{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import torch_geometric.nn as pyg_nn \n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import os.path as osp\n",
    "import scipy.io as sio\n",
    "from dataset import QUASARDataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "dir = '/Users/hankyang/Datasets/QUASAR'\n",
    "dataset = QUASARDataset(dir)\n",
    "data = dataset[0]\n",
    "# X = torch.rand((5,5))\n",
    "# idx = np.arange(0,4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "def smat(x,n):\n",
    "    X        = torch.zeros(n,n,dtype=torch.float64)\n",
    "    si, sj   = torch.triu_indices(n,n)\n",
    "    X[si,sj] = x \n",
    "    X        = X + X.t() # symmetric matrix\n",
    "    return X\n",
    "\n",
    "def mat(x,n):\n",
    "    return x.view((n,n))\n",
    "\n",
    "def blk_index(i,p):\n",
    "    return np.arange(i*p, (i+1)*p)\n",
    "\n",
    "def recover_X(vp,ep,ud_edges):\n",
    "    N = vp.shape[0] # number of nodes\n",
    "    n = 4*N # size of symmetric X\n",
    "    X = torch.zeros((n,n),dtype=torch.float64)\n",
    "    # assign diagonal blocks\n",
    "    for i in range(N):\n",
    "        idx = blk_index(i,4)\n",
    "        X[idx,:][:,idx] = smat(vp[i,:],4)\n",
    "    # assign off-diagonal blocks \n",
    "    # for edge_idx, edge in enumerate(ud_edges):\n",
    "    #     blk   = smat(ep[edge_idx,:],4)\n",
    "    #     nodei = edge[0]\n",
    "    #     nodej = edge[1]\n",
    "    #     idx_i = blk_index(nodei,4)\n",
    "    #     idx_j = blk_index(nodej,4)\n",
    "\n",
    "    #     X[idx_i,:][:,idx_j] = blk\n",
    "    #     X[idx_j,:][:,idx_i] = blk\n",
    "    return X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    " \n",
    "class ModelS(nn.Module):\n",
    "    def __init__(self, mp_input_dim=6,\n",
    "                       mp_hidden_dim=32,\n",
    "                       mp_output_dim=64,\n",
    "                       mp_num_layers=1, \n",
    "                       primal_node_mlp_hidden_dim=64,\n",
    "                       primal_node_mlp_output_dim=10,\n",
    "                       dual_node_mlp_hidden_dim=64,\n",
    "                       dual_node_mlp_output_dim=10,\n",
    "                       node_mlp_num_layers=1,\n",
    "                       primal_edge_mlp_hidden_dim=64, \n",
    "                       primal_edge_mlp_output_dim=10, \n",
    "                       dual_edge_mlp_hidden_dim=64, \n",
    "                       dual_edge_mlp_output_dim=16, \n",
    "                       edge_mlp_num_layers=1, \n",
    "                       dropout_rate=0.2):\n",
    "        super(ModelS,self).__init__()\n",
    "        # Message passing\n",
    "        self.mp_convs = nn.ModuleList()\n",
    "        self.mp_convs.append(pyg_nn.SAGEConv(mp_input_dim,mp_hidden_dim))\n",
    "        for i in range(mp_num_layers):\n",
    "            self.mp_convs.append(pyg_nn.SAGEConv(mp_hidden_dim,mp_hidden_dim))\n",
    "        self.mp_convs.append(pyg_nn.SAGEConv(mp_hidden_dim,mp_output_dim))\n",
    "\n",
    "        # Post message passing\n",
    "        # Primal node MLP\n",
    "        self.primal_node_mlp = nn.ModuleList()\n",
    "        self.primal_node_mlp.append(\n",
    "            nn.Linear(mp_output_dim,primal_node_mlp_hidden_dim,dtype=torch.float64))\n",
    "        for i in range(node_mlp_num_layers):\n",
    "            self.primal_node_mlp.append(\n",
    "                nn.Linear(primal_node_mlp_hidden_dim,primal_node_mlp_hidden_dim,dtype=torch.float64))\n",
    "        self.primal_node_mlp.append(\n",
    "            nn.Linear(primal_node_mlp_hidden_dim,primal_node_mlp_output_dim,dtype=torch.float64))\n",
    "        # Dual node MLP\n",
    "        self.dual_node_mlp = nn.ModuleList()\n",
    "        self.dual_node_mlp.append(\n",
    "            nn.Linear(mp_output_dim,dual_node_mlp_hidden_dim,dtype=torch.float64))\n",
    "        for i in range(node_mlp_num_layers):\n",
    "            self.dual_node_mlp.append(\n",
    "                nn.Linear(dual_node_mlp_hidden_dim,dual_node_mlp_hidden_dim,dtype=torch.float64))\n",
    "        self.dual_node_mlp.append(\n",
    "            nn.Linear(dual_node_mlp_hidden_dim,dual_node_mlp_output_dim,dtype=torch.float64))\n",
    "        # Primal edge MLP\n",
    "        self.primal_edge_mlp = nn.ModuleList()\n",
    "        self.primal_edge_mlp.append(\n",
    "            nn.Linear(mp_output_dim,primal_edge_mlp_hidden_dim,dtype=torch.float64))\n",
    "        for i in range(edge_mlp_num_layers):\n",
    "            self.primal_edge_mlp.append(\n",
    "                nn.Linear(primal_edge_mlp_hidden_dim,primal_edge_mlp_hidden_dim,dtype=torch.float64))\n",
    "        self.primal_edge_mlp.append(\n",
    "            nn.Linear(primal_edge_mlp_hidden_dim,primal_edge_mlp_output_dim,dtype=torch.float64))\n",
    "        # Dual edge MLP\n",
    "        self.dual_edge_mlp = nn.ModuleList()\n",
    "        self.dual_edge_mlp.append(\n",
    "            nn.Linear(mp_output_dim,dual_edge_mlp_hidden_dim,dtype=torch.float64))\n",
    "        for i in range(edge_mlp_num_layers):\n",
    "            self.dual_edge_mlp.append(\n",
    "                nn.Linear(dual_edge_mlp_hidden_dim,dual_edge_mlp_hidden_dim,dtype=torch.float64))\n",
    "        self.dual_edge_mlp.append(\n",
    "            nn.Linear(dual_edge_mlp_hidden_dim,dual_edge_mlp_output_dim,dtype=torch.float64))\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self,data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        print(x.type())\n",
    "        num_nodes = data.num_nodes\n",
    "        ud_edges  = data.ud_edges\n",
    "        # Message passing\n",
    "        for mp_layer in self.mp_convs:\n",
    "            x = mp_layer(x,edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x,p=self.dropout_rate,training=self.training)\n",
    "        \n",
    "        # Post message passing\n",
    "        # Primal node\n",
    "        vp = []\n",
    "        for i in range(num_nodes):\n",
    "            xi = x[i,:] # feature of i-th node\n",
    "            for mlp_layer in self.primal_node_mlp:\n",
    "                xi = mlp_layer(xi)\n",
    "                xi = F.relu(xi)\n",
    "                xi = F.dropout(xi,p=self.dropout_rate,training=self.training)\n",
    "            vp.append(xi)\n",
    "        vp = torch.stack(vp) # num_nodes x primal_node_mlp_output_dim\n",
    "        # Dual node\n",
    "        vd = []\n",
    "        for i in range(num_nodes):\n",
    "            xi = x[i,:]\n",
    "            for mlp_layer in self.dual_node_mlp:\n",
    "                xi = mlp_layer(xi)\n",
    "                xi = F.relu(xi)\n",
    "                xi = F.dropout(xi,p=self.dropout_rate,training=self.training)\n",
    "            vd.append(xi)\n",
    "        vd = torch.stack(vd) # num_nodes x dual_node_mlp_output_dim\n",
    "        # Primal edge\n",
    "        ep = []\n",
    "        for edge in ud_edges:\n",
    "            xi  = x[edge[0],:]\n",
    "            xj  = x[edge[1],:]\n",
    "            xij = xi + xj\n",
    "            for mlp_layer in self.primal_edge_mlp:\n",
    "                xij = mlp_layer(xij)\n",
    "                xij = F.relu(xij)\n",
    "                xij = F.dropout(xij,p=self.dropout_rate,training=self.training)\n",
    "            ep.append(xij)\n",
    "        ep = torch.stack(ep)\n",
    "        # Dual edge\n",
    "        ed = []\n",
    "        for edge in ud_edges:\n",
    "            xi  = x[edge[0],:]\n",
    "            xj  = x[edge[1],:]\n",
    "            xij = xi + xj\n",
    "            for mlp_layer in self.dual_edge_mlp:\n",
    "                xij = mlp_layer(xij)\n",
    "                xij = F.relu(xij)\n",
    "                xij = F.dropout(xij,p=self.dropout_rate,training=self.training)\n",
    "            ed.append(xij)\n",
    "        ed = torch.stack(ed)\n",
    "\n",
    "        return x, vp, vd, ep, ed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# Test evaluate model\n",
    "model = ModelS()\n",
    "model.double()\n",
    "model.eval()\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)\n",
    "#         print(param.data)\n",
    "x, vp, vd, ep, ed = model(data)\n",
    "# print(smat(vp[0,:],4))\n",
    "# X = torch.zeros((84,84))\n",
    "# X[0:4,:][:,0:4] = smat(vp[0,:],4)\n",
    "# X[4:8,:][:,4:8] = smat(vp[1,:],4)\n",
    "X = recover_X(vp,ep,data.ud_edges)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.DoubleTensor\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.13 64-bit ('gnnsdp': conda)"
  },
  "interpreter": {
   "hash": "fb02ce160b1ce38f1f8273f7fbdcd1a85000978aa7db8bbd86770a5b36cc1e21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}