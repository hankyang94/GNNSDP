{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import torch_geometric.nn as pyg_nn \n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "import torch.optim as optim\n",
    "import os.path as osp\n",
    "import scipy.io as sio\n",
    "from datetime import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "from dataset import QUASARDataset\n",
    "from model import ModelS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = '/Users/hankyang/Datasets/QUASAR'\n",
    "dir = '/home/hank/Datasets/QUASAR'\n",
    "dataset = QUASARDataset(dir)\n",
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader  = DataLoader(dataset,batch_size=1,shuffle=True)\n",
    "def train(dataset,writer):\n",
    "    # build model\n",
    "    model   = ModelS(mp_input_dim=6,mp_hidden_dim=32,mp_output_dim=64,mp_num_layers=1, \n",
    "                     primal_node_mlp_hidden_dim=64,primal_node_mlp_output_dim=10,\n",
    "                     dual_node_mlp_hidden_dim=64,dual_node_mlp_output_dim=10,\n",
    "                     node_mlp_num_layers=1,\n",
    "                     primal_edge_mlp_hidden_dim=64,primal_edge_mlp_output_dim=10, \n",
    "                     dual_edge_mlp_hidden_dim=64,dual_edge_mlp_output_dim=6, \n",
    "                     edge_mlp_num_layers=1, \n",
    "                     dropout_rate=0.2)\n",
    "    model.double() # convert all parameters to double\n",
    "    model.to(device)\n",
    "    opt = optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "    # train\n",
    "    num_epoches = 10\n",
    "    for epoch in range(num_epoches):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for i in range(len(dataset)):\n",
    "            opt.zero_grad()\n",
    "            data = dataset[i].to(device)\n",
    "            x, X, S, Aty = model(data)\n",
    "            loss = model.loss(data,X,S,Aty)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "            print('graph {}. loss: {:.4f}.'.format(i,loss.item()))\n",
    "        total_loss /= len(dataset)\n",
    "        writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "        print(\"Epoch {}. Loss: {:.4f}.\".format(epoch, total_loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph 0. loss: 11564.0055.\n",
      "graph 1. loss: 11479.8635.\n",
      "graph 2. loss: 11375.2209.\n",
      "graph 3. loss: 11479.7515.\n",
      "graph 4. loss: 11407.2278.\n",
      "graph 5. loss: 11484.6304.\n",
      "graph 6. loss: 11556.8459.\n",
      "graph 7. loss: 11607.1547.\n",
      "graph 8. loss: 11517.2378.\n",
      "graph 9. loss: 11489.4784.\n",
      "graph 10. loss: 11629.6318.\n",
      "graph 11. loss: 11425.0710.\n",
      "graph 12. loss: 11361.8278.\n",
      "graph 13. loss: 11536.2976.\n",
      "graph 14. loss: 11566.8950.\n",
      "graph 15. loss: 11442.8712.\n",
      "graph 16. loss: 11465.8513.\n",
      "graph 17. loss: 11467.0776.\n",
      "graph 18. loss: 11667.9359.\n",
      "graph 19. loss: 11466.3238.\n",
      "graph 20. loss: 11489.5964.\n",
      "graph 21. loss: 11567.1923.\n",
      "graph 22. loss: 11616.5023.\n",
      "graph 23. loss: 11434.3724.\n",
      "graph 24. loss: 11459.6099.\n",
      "graph 25. loss: 11425.7042.\n",
      "graph 26. loss: 11655.6241.\n",
      "graph 27. loss: 11325.9890.\n",
      "graph 28. loss: 11889.2223.\n",
      "graph 29. loss: 11532.0765.\n",
      "graph 30. loss: 11799.4028.\n",
      "graph 31. loss: 11520.3834.\n",
      "graph 32. loss: 11436.6303.\n",
      "graph 33. loss: 11531.4156.\n",
      "graph 34. loss: 11396.4439.\n",
      "graph 35. loss: 11467.1679.\n",
      "graph 36. loss: 11474.1412.\n",
      "graph 37. loss: 11387.5736.\n",
      "graph 38. loss: 11472.5447.\n",
      "graph 39. loss: 11384.8833.\n",
      "graph 40. loss: 11461.5972.\n",
      "graph 41. loss: 11485.9732.\n",
      "graph 42. loss: 11425.1075.\n",
      "graph 43. loss: 11549.1315.\n",
      "graph 44. loss: 11516.8964.\n",
      "graph 45. loss: 11506.0488.\n",
      "graph 46. loss: 11513.8779.\n",
      "graph 47. loss: 11478.3101.\n",
      "graph 48. loss: 11471.1066.\n",
      "graph 49. loss: 11452.3668.\n",
      "graph 50. loss: 11398.6496.\n",
      "graph 51. loss: 11461.0812.\n",
      "graph 52. loss: 11544.7344.\n",
      "graph 53. loss: 11410.1926.\n",
      "graph 54. loss: 11535.0973.\n",
      "graph 55. loss: 11524.7881.\n",
      "graph 56. loss: 11777.2899.\n",
      "graph 57. loss: 11470.6789.\n",
      "graph 58. loss: 11630.0701.\n",
      "graph 59. loss: 11434.6240.\n",
      "graph 60. loss: 11525.1082.\n",
      "graph 61. loss: 11479.5812.\n",
      "graph 62. loss: 11523.6084.\n",
      "graph 63. loss: 11635.1227.\n",
      "graph 64. loss: 11490.2795.\n",
      "graph 65. loss: 11439.0317.\n",
      "graph 66. loss: 11522.1092.\n",
      "graph 67. loss: 11418.3895.\n",
      "graph 68. loss: 11462.1525.\n",
      "graph 69. loss: 11361.2814.\n",
      "graph 70. loss: 11467.4451.\n",
      "graph 71. loss: 11429.9417.\n",
      "graph 72. loss: 11575.4159.\n",
      "graph 73. loss: 11499.2425.\n",
      "graph 74. loss: 11458.3918.\n",
      "graph 75. loss: 11526.2207.\n",
      "graph 76. loss: 11445.5815.\n",
      "graph 77. loss: 11402.8913.\n",
      "graph 78. loss: 11495.9165.\n",
      "graph 79. loss: 11479.0133.\n",
      "graph 80. loss: 11440.6324.\n",
      "graph 81. loss: 11415.4637.\n",
      "graph 82. loss: 11442.2087.\n",
      "graph 83. loss: 11335.8278.\n",
      "graph 84. loss: 11412.1355.\n",
      "graph 85. loss: 11583.8417.\n",
      "graph 86. loss: 11470.1574.\n",
      "graph 87. loss: 11555.6713.\n",
      "graph 88. loss: 11533.2563.\n",
      "graph 89. loss: 11429.6460.\n",
      "graph 90. loss: 11421.6541.\n",
      "graph 91. loss: 11375.0654.\n",
      "graph 92. loss: 11409.6370.\n",
      "graph 93. loss: 11471.5826.\n",
      "graph 94. loss: 11583.3177.\n",
      "graph 95. loss: 11450.5679.\n",
      "graph 96. loss: 11430.6996.\n",
      "graph 97. loss: 11556.1216.\n",
      "graph 98. loss: 11384.0266.\n",
      "graph 99. loss: 11543.7213.\n",
      "graph 100. loss: 11347.2500.\n",
      "graph 101. loss: 11421.3199.\n",
      "graph 102. loss: 11308.4139.\n",
      "graph 103. loss: 11234.1489.\n",
      "graph 104. loss: 11425.6897.\n",
      "graph 105. loss: 11135.4516.\n",
      "graph 106. loss: 11417.3264.\n",
      "graph 107. loss: 11370.5660.\n",
      "graph 108. loss: 11419.3202.\n",
      "graph 109. loss: 11297.6343.\n",
      "graph 110. loss: 11220.3398.\n",
      "graph 111. loss: 11198.6161.\n",
      "graph 112. loss: 11257.3830.\n",
      "graph 113. loss: 11127.2250.\n",
      "graph 114. loss: 11095.8040.\n",
      "graph 115. loss: 11137.2963.\n",
      "graph 116. loss: 11172.5028.\n",
      "graph 117. loss: 11268.8650.\n",
      "graph 118. loss: 11185.2849.\n",
      "graph 119. loss: 11209.6753.\n",
      "graph 120. loss: 11132.3659.\n",
      "graph 121. loss: 11163.9879.\n",
      "graph 122. loss: 11405.5454.\n",
      "graph 123. loss: 11305.5945.\n",
      "graph 124. loss: 11220.2373.\n",
      "graph 125. loss: 11131.1901.\n",
      "graph 126. loss: 11335.3527.\n",
      "graph 127. loss: 11078.7258.\n",
      "graph 128. loss: 11311.6839.\n",
      "graph 129. loss: 11390.8404.\n",
      "graph 130. loss: 11333.2539.\n",
      "graph 131. loss: 11324.9504.\n",
      "graph 132. loss: 11318.0393.\n",
      "graph 133. loss: 11238.1846.\n",
      "graph 134. loss: 11115.4188.\n",
      "graph 135. loss: 11517.5938.\n",
      "graph 136. loss: 11268.3931.\n",
      "graph 137. loss: 11328.5703.\n",
      "graph 138. loss: 11255.9845.\n",
      "graph 139. loss: 11083.8757.\n",
      "graph 140. loss: 10852.0699.\n",
      "graph 141. loss: 11114.1359.\n",
      "graph 142. loss: 11244.2577.\n",
      "graph 143. loss: 11171.4566.\n",
      "graph 144. loss: 10929.9581.\n",
      "graph 145. loss: 11395.3853.\n",
      "graph 146. loss: 11064.0925.\n",
      "graph 147. loss: 10878.0315.\n",
      "graph 148. loss: 10802.3916.\n",
      "graph 149. loss: 11105.5568.\n",
      "graph 150. loss: 11301.7472.\n",
      "graph 151. loss: 11260.5236.\n",
      "graph 152. loss: 11067.4008.\n",
      "graph 153. loss: 11279.0916.\n",
      "graph 154. loss: 11118.9864.\n",
      "graph 155. loss: 10706.4643.\n",
      "graph 156. loss: 11064.5208.\n",
      "graph 157. loss: 11232.6686.\n",
      "graph 158. loss: 10332.3340.\n",
      "graph 159. loss: 10401.6104.\n",
      "graph 160. loss: 9514.3685.\n",
      "graph 161. loss: 12586.6655.\n",
      "graph 162. loss: 10733.0000.\n",
      "graph 163. loss: 10911.6017.\n",
      "graph 164. loss: 11422.8172.\n",
      "graph 165. loss: 11271.9769.\n",
      "graph 166. loss: 10497.9286.\n",
      "graph 167. loss: 11222.3900.\n",
      "graph 168. loss: 11270.3764.\n",
      "graph 169. loss: 10375.7236.\n",
      "graph 170. loss: 10347.6108.\n",
      "graph 171. loss: 10011.3071.\n",
      "graph 172. loss: 10077.9522.\n",
      "graph 173. loss: 10144.2717.\n",
      "graph 174. loss: 11943.6357.\n",
      "graph 175. loss: 9700.0361.\n",
      "graph 176. loss: 9691.9808.\n",
      "graph 177. loss: 12054.6561.\n",
      "graph 178. loss: 9125.3087.\n",
      "graph 179. loss: 11383.8104.\n",
      "graph 180. loss: 10900.9665.\n",
      "graph 181. loss: 10191.4996.\n",
      "graph 182. loss: 10767.3332.\n",
      "graph 183. loss: 11352.6495.\n",
      "graph 184. loss: 10589.8760.\n",
      "graph 185. loss: 11313.4425.\n",
      "graph 186. loss: 11147.3680.\n",
      "graph 187. loss: 10657.9771.\n",
      "graph 188. loss: 10353.5209.\n",
      "graph 189. loss: 10497.0733.\n",
      "graph 190. loss: 10388.7865.\n",
      "graph 191. loss: 9349.0951.\n",
      "graph 192. loss: 9585.7221.\n",
      "graph 193. loss: 10841.7384.\n",
      "graph 194. loss: 9911.5883.\n",
      "graph 195. loss: 11294.7808.\n",
      "graph 196. loss: 10427.1705.\n",
      "graph 197. loss: 9615.8740.\n",
      "graph 198. loss: 11389.0414.\n",
      "graph 199. loss: 9515.1155.\n",
      "graph 200. loss: 10849.6363.\n",
      "graph 201. loss: 10999.0103.\n",
      "graph 202. loss: 9363.5071.\n",
      "graph 203. loss: 10017.6709.\n",
      "graph 204. loss: 11048.3274.\n",
      "graph 205. loss: 10875.0405.\n",
      "graph 206. loss: 8812.7746.\n",
      "graph 207. loss: 11005.6328.\n",
      "graph 208. loss: 10430.4639.\n",
      "graph 209. loss: 10512.4384.\n",
      "graph 210. loss: 9593.1641.\n",
      "graph 211. loss: 9557.1615.\n",
      "graph 212. loss: 12261.9986.\n",
      "graph 213. loss: 10240.7784.\n",
      "graph 214. loss: 9775.8683.\n",
      "graph 215. loss: 9927.4877.\n",
      "graph 216. loss: 11071.7877.\n",
      "graph 217. loss: 9418.1915.\n",
      "graph 218. loss: 9273.9830.\n",
      "graph 219. loss: 11292.6086.\n",
      "graph 220. loss: 10263.5560.\n",
      "graph 221. loss: 10914.8138.\n",
      "graph 222. loss: 9903.1206.\n",
      "graph 223. loss: 11248.6138.\n",
      "graph 224. loss: 10418.4075.\n",
      "graph 225. loss: 9679.0089.\n",
      "graph 226. loss: 10800.2714.\n",
      "graph 227. loss: 11081.0840.\n",
      "graph 228. loss: 9335.1271.\n",
      "graph 229. loss: 10254.6494.\n",
      "graph 230. loss: 9387.7541.\n",
      "graph 231. loss: 11860.3882.\n",
      "graph 232. loss: 8505.5969.\n",
      "graph 233. loss: 8762.9440.\n",
      "graph 234. loss: 8851.2580.\n",
      "graph 235. loss: 8040.9366.\n",
      "graph 236. loss: 9631.7485.\n",
      "graph 237. loss: 10683.5706.\n",
      "graph 238. loss: 7779.9302.\n",
      "graph 239. loss: 7996.4555.\n",
      "graph 240. loss: 10620.4950.\n",
      "graph 241. loss: 9657.8771.\n",
      "graph 242. loss: 10500.8513.\n",
      "graph 243. loss: 10087.1409.\n",
      "graph 244. loss: 9308.8542.\n",
      "graph 245. loss: 8003.6914.\n",
      "graph 246. loss: 7657.1086.\n",
      "graph 247. loss: 9416.8874.\n",
      "graph 248. loss: 9112.1104.\n",
      "graph 249. loss: 9555.3303.\n",
      "graph 250. loss: 7316.3274.\n",
      "graph 251. loss: 10488.4770.\n",
      "graph 252. loss: 9268.8215.\n",
      "graph 253. loss: 9466.1328.\n",
      "graph 254. loss: 9037.5638.\n",
      "graph 255. loss: 9323.1193.\n",
      "graph 256. loss: 9650.1788.\n",
      "graph 257. loss: 9726.5922.\n",
      "graph 258. loss: 8063.6594.\n",
      "graph 259. loss: 8267.5805.\n",
      "graph 260. loss: 8160.9378.\n",
      "graph 261. loss: 10911.8809.\n",
      "graph 262. loss: 12601.0037.\n",
      "graph 263. loss: 8877.4793.\n",
      "graph 264. loss: 10256.7717.\n",
      "graph 265. loss: 11272.5885.\n",
      "graph 266. loss: 9874.8801.\n",
      "graph 267. loss: 10837.9295.\n",
      "graph 268. loss: 8425.2046.\n",
      "graph 269. loss: 8462.3777.\n",
      "graph 270. loss: 7707.0706.\n",
      "graph 271. loss: 7837.8527.\n",
      "graph 272. loss: 7358.7776.\n",
      "graph 273. loss: 7592.7936.\n",
      "graph 274. loss: 10124.3133.\n",
      "graph 275. loss: 9544.6553.\n",
      "graph 276. loss: 10327.3495.\n",
      "graph 277. loss: 10181.7273.\n",
      "graph 278. loss: 7563.6694.\n",
      "graph 279. loss: 9834.8247.\n",
      "graph 280. loss: 9013.9676.\n",
      "graph 281. loss: 11038.9316.\n",
      "graph 282. loss: 7508.7793.\n",
      "graph 283. loss: 8426.7783.\n",
      "graph 284. loss: 11146.2895.\n",
      "graph 285. loss: 7834.7074.\n",
      "graph 286. loss: 8767.8028.\n",
      "graph 287. loss: 9697.7348.\n",
      "graph 288. loss: 11045.5975.\n",
      "graph 289. loss: 7811.0175.\n",
      "graph 290. loss: 9262.5766.\n",
      "graph 291. loss: 10623.1631.\n",
      "graph 292. loss: 10566.0735.\n",
      "graph 293. loss: 10830.5014.\n",
      "graph 294. loss: 11127.4101.\n",
      "graph 295. loss: 9349.3597.\n",
      "graph 296. loss: 9852.0827.\n",
      "graph 297. loss: 10099.4642.\n",
      "graph 298. loss: 9331.8263.\n",
      "graph 299. loss: 11505.9935.\n",
      "graph 300. loss: 10698.4487.\n",
      "graph 301. loss: 8377.2414.\n",
      "graph 302. loss: 8155.7354.\n",
      "graph 303. loss: 7708.2975.\n",
      "graph 304. loss: 8759.5068.\n",
      "graph 305. loss: 9911.2297.\n",
      "graph 306. loss: 10517.0152.\n",
      "graph 307. loss: 8486.3120.\n",
      "graph 308. loss: 8032.3025.\n",
      "graph 309. loss: 8750.7854.\n",
      "graph 310. loss: 9981.9398.\n",
      "graph 311. loss: 8115.5849.\n",
      "graph 312. loss: 9280.0739.\n",
      "graph 313. loss: 8889.9540.\n",
      "graph 314. loss: 8900.7140.\n",
      "graph 315. loss: 9496.2560.\n",
      "graph 316. loss: 10897.3148.\n",
      "graph 317. loss: 10383.2065.\n",
      "graph 318. loss: 8620.1870.\n",
      "graph 319. loss: 7118.2606.\n",
      "graph 320. loss: 10215.3890.\n",
      "graph 321. loss: 7494.3795.\n",
      "graph 322. loss: 9737.6442.\n",
      "graph 323. loss: 9672.9978.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hank/Code/GNNSDP/quasar_gnn.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.47.5.244/home/hank/Code/GNNSDP/quasar_gnn.ipynb#ch0000006vscode-remote?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m train(dataset,writer)\n",
      "\u001b[1;32m/home/hank/Code/GNNSDP/quasar_gnn.ipynb Cell 3'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, writer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.47.5.244/home/hank/Code/GNNSDP/quasar_gnn.ipynb#ch0000002vscode-remote?line=23'>24</a>\u001b[0m x, X, S, Aty \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.47.5.244/home/hank/Code/GNNSDP/quasar_gnn.ipynb#ch0000002vscode-remote?line=24'>25</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss(data,X,S,Aty)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.47.5.244/home/hank/Code/GNNSDP/quasar_gnn.ipynb#ch0000002vscode-remote?line=25'>26</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.47.5.244/home/hank/Code/GNNSDP/quasar_gnn.ipynb#ch0000002vscode-remote?line=26'>27</a>\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.47.5.244/home/hank/Code/GNNSDP/quasar_gnn.ipynb#ch0000002vscode-remote?line=27'>28</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train(dataset,writer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
