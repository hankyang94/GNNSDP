{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import torch_geometric.nn as pyg_nn \n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import os.path as osp\n",
    "import scipy.io as sio\n",
    "from dataset import QUASARDataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "dir = '/Users/hankyang/Datasets/QUASAR'\n",
    "dataset = QUASARDataset(dir)\n",
    "data = dataset[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "class ModelS(nn.Module):\n",
    "    def __init__(self, mp_input_dim=6,\n",
    "                       mp_hidden_dim=32,\n",
    "                       mp_output_dim=64,\n",
    "                       mp_num_layers=1, \n",
    "                       primal_node_mlp_hidden_dim=64,\n",
    "                       primal_node_mlp_output_dim=10,\n",
    "                       dual_node_mlp_hidden_dim=64,\n",
    "                       dual_node_mlp_output_dim=10,\n",
    "                       node_mlp_num_layers=1,\n",
    "                       primal_edge_mlp_hidden_dim=64, \n",
    "                       primal_edge_mlp_output_dim=10, \n",
    "                       dual_edge_mlp_hidden_dim=64, \n",
    "                       dual_edge_mlp_output_dim=16, \n",
    "                       edge_mlp_num_layers=1, \n",
    "                       dropout_rate=0.2):\n",
    "        super(ModelS,self).__init__()\n",
    "        # Message passing\n",
    "        self.mp_convs = nn.ModuleList()\n",
    "        self.mp_convs.append(pyg_nn.SAGEConv(mp_input_dim,mp_hidden_dim))\n",
    "        for i in range(mp_num_layers):\n",
    "            self.mp_convs.append(pyg_nn.SAGEConv(mp_hidden_dim,mp_hidden_dim))\n",
    "        self.mp_convs.append(pyg_nn.SAGEConv(mp_hidden_dim,mp_output_dim))\n",
    "\n",
    "        # Post message passing\n",
    "        # Primal node MLP\n",
    "        self.primal_node_mlp = nn.ModuleList()\n",
    "        self.primal_node_mlp.append(\n",
    "            nn.Linear(mp_output_dim,primal_node_mlp_hidden_dim,dtype=torch.float64))\n",
    "        for i in range(node_mlp_num_layers):\n",
    "            self.primal_node_mlp.append(\n",
    "                nn.Linear(primal_node_mlp_hidden_dim,primal_node_mlp_hidden_dim,dtype=torch.float64))\n",
    "        self.primal_node_mlp.append(\n",
    "            nn.Linear(primal_node_mlp_hidden_dim,primal_node_mlp_output_dim,dtype=torch.float64))\n",
    "        # Dual node MLP\n",
    "        self.dual_node_mlp = nn.ModuleList()\n",
    "        self.dual_node_mlp.append(\n",
    "            nn.Linear(mp_output_dim,dual_node_mlp_hidden_dim,dtype=torch.float64))\n",
    "        for i in range(node_mlp_num_layers):\n",
    "            self.dual_node_mlp.append(\n",
    "                nn.Linear(dual_node_mlp_hidden_dim,dual_node_mlp_hidden_dim,dtype=torch.float64))\n",
    "        self.dual_node_mlp.append(\n",
    "            nn.Linear(dual_node_mlp_hidden_dim,dual_node_mlp_output_dim,dtype=torch.float64))\n",
    "        # Primal edge MLP\n",
    "        self.primal_edge_mlp = nn.ModuleList()\n",
    "        self.primal_edge_mlp.append(\n",
    "            nn.Linear(mp_output_dim,primal_edge_mlp_hidden_dim,dtype=torch.float64))\n",
    "        for i in range(edge_mlp_num_layers):\n",
    "            self.primal_edge_mlp.append(\n",
    "                nn.Linear(primal_edge_mlp_hidden_dim,primal_edge_mlp_hidden_dim,dtype=torch.float64))\n",
    "        self.primal_edge_mlp.append(\n",
    "            nn.Linear(primal_edge_mlp_hidden_dim,primal_edge_mlp_output_dim,dtype=torch.float64))\n",
    "        # Dual edge MLP\n",
    "        self.dual_edge_mlp = nn.ModuleList()\n",
    "        self.dual_edge_mlp.append(\n",
    "            nn.Linear(mp_output_dim,dual_edge_mlp_hidden_dim,dtype=torch.float64))\n",
    "        for i in range(edge_mlp_num_layers):\n",
    "            self.dual_edge_mlp.append(\n",
    "                nn.Linear(dual_edge_mlp_hidden_dim,dual_edge_mlp_hidden_dim,dtype=torch.float64))\n",
    "        self.dual_edge_mlp.append(\n",
    "            nn.Linear(dual_edge_mlp_hidden_dim,dual_edge_mlp_output_dim,dtype=torch.float64))\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self,data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        num_nodes = data.num_nodes\n",
    "        ud_edges  = data.ud_edges\n",
    "        edge_map  = data.edge_map\n",
    "        # Message passing\n",
    "        for mp_layer in self.mp_convs:\n",
    "            x = mp_layer(x,edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x,p=self.dropout_rate,training=self.training)\n",
    "        \n",
    "        # Post message passing\n",
    "        # Primal node\n",
    "        vp = []\n",
    "        for i in range(num_nodes):\n",
    "            xi = x[i,:] # feature of i-th node\n",
    "            for mlp_layer in self.primal_node_mlp:\n",
    "                xi = mlp_layer(xi)\n",
    "                xi = F.relu(xi)\n",
    "                xi = F.dropout(xi,p=self.dropout_rate,training=self.training)\n",
    "            vp.append(xi)\n",
    "        vp = torch.stack(vp) # num_nodes x primal_node_mlp_output_dim\n",
    "        # Dual node\n",
    "        vd = []\n",
    "        for i in range(num_nodes):\n",
    "            xi = x[i,:]\n",
    "            for mlp_layer in self.dual_node_mlp:\n",
    "                xi = mlp_layer(xi)\n",
    "                xi = F.relu(xi)\n",
    "                xi = F.dropout(xi,p=self.dropout_rate,training=self.training)\n",
    "            vd.append(xi)\n",
    "        vd = torch.stack(vd) # num_nodes x dual_node_mlp_output_dim\n",
    "        # Primal edge\n",
    "        ep = []\n",
    "        for edge in ud_edges:\n",
    "            xi  = x[edge[0],:]\n",
    "            xj  = x[edge[1],:]\n",
    "            xij = xi + xj\n",
    "            for mlp_layer in self.primal_edge_mlp:\n",
    "                xij = mlp_layer(xij)\n",
    "                xij = F.relu(xij)\n",
    "                xij = F.dropout(xij,p=self.dropout_rate,training=self.training)\n",
    "            ep.append(xij)\n",
    "        ep = torch.stack(ep)\n",
    "        # Dual edge\n",
    "        ed = []\n",
    "        for edge in ud_edges:\n",
    "            xi  = x[edge[0],:]\n",
    "            xj  = x[edge[1],:]\n",
    "            xij = xi + xj\n",
    "            for mlp_layer in self.dual_edge_mlp:\n",
    "                xij = mlp_layer(xij)\n",
    "                xij = F.relu(xij)\n",
    "                xij = F.dropout(xij,p=self.dropout_rate,training=self.training)\n",
    "            ed.append(xij)\n",
    "        ed = torch.stack(ed)\n",
    "\n",
    "        # Recover primal X\n",
    "        X = recover_X(vp,ep,edge_map)\n",
    "        print(X[0,0].grad_fn)\n",
    "        print(x.grad_fn)\n",
    "        return x, X\n",
    "\n",
    "    def smat(self,x):\n",
    "        X = torch.tensor([[x[0],x[1],x[2],x[3]],\n",
    "                          [x[1],x[4],x[5],x[6]], \n",
    "                          [x[2],x[5],x[7],x[8]], \n",
    "                          [x[3],x[6],x[8],x[9]]],dtype=torch.float64)\n",
    "        return X\n",
    "\n",
    "    def mat(self,x,n):\n",
    "        return x.view((n,n))\n",
    "\n",
    "    def recover_X(self,vp,ep,edge_map):\n",
    "        N = vp.shape[0] # number of nodes\n",
    "        rows = []\n",
    "        for i in range(N):\n",
    "            row = []\n",
    "            for j in range(N):\n",
    "                if i == j: # diagonal blocks, using node features vp\n",
    "                    blk = smat(vp[i,:])\n",
    "                else: # off-diagonal blocks, using edge features ep\n",
    "                    edge_id = edge_map[i,j]\n",
    "                    blk = smat(ep[edge_id,:])\n",
    "                row.append(blk)\n",
    "            row_mat = torch.cat(row,dim=1)\n",
    "            rows.append(row_mat)\n",
    "        X = torch.cat(rows,dim=0)\n",
    "        return X\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Test evaluate model\n",
    "model = ModelS()\n",
    "model.double()\n",
    "model.eval()\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)\n",
    "#         print(param.data)\n",
    "x, X = model(data)\n",
    "print(torch.norm(X-X.t(),p='fro'))\n",
    "print(X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n",
      "<ReluBackward0 object at 0x7fc060500430>\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor([[0.0000, 0.0300, 0.0000,  ..., 0.0000, 0.0974, 0.0959],\n",
      "        [0.0300, 0.0986, 0.0489,  ..., 0.1042, 0.0000, 0.0460],\n",
      "        [0.0000, 0.0489, 0.0000,  ..., 0.0000, 0.0000, 0.0578],\n",
      "        ...,\n",
      "        [0.0000, 0.1042, 0.0000,  ..., 0.1001, 0.0482, 0.1189],\n",
      "        [0.0974, 0.0000, 0.0000,  ..., 0.0482, 0.0000, 0.1006],\n",
      "        [0.0959, 0.0460, 0.0578,  ..., 0.1189, 0.1006, 0.1263]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.13 64-bit ('gnnsdp': conda)"
  },
  "interpreter": {
   "hash": "fb02ce160b1ce38f1f8273f7fbdcd1a85000978aa7db8bbd86770a5b36cc1e21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}