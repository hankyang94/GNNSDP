{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import torch_geometric.nn as pyg_nn \n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "import torch.optim as optim\n",
    "import os.path as osp\n",
    "import scipy.io as sio\n",
    "from dataset import QUASARDataset\n",
    "from dual_model import DualModelFtype\n",
    "from dual_train import validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: node_feature_mode = 1, mp_input_dim = 6, relu_slope = 0.1. GNN type: SAGE. Residual: True. BatchNorm: True.\n",
      "Data graph type: 1.\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "DEVICE = torch.device('cuda:1')\n",
    "# Model\n",
    "GNN_TYPE = 'SAGE'\n",
    "GNN_HIDDEN_DIM = 64\n",
    "GNN_OUT_DIM = GNN_HIDDEN_DIM\n",
    "GNN_LAYER = 15\n",
    "NODE_MODE = 1\n",
    "DATA_GRAPH_TYPE = 1\n",
    "DROPOUT = 0.2\n",
    "MLP_LAYER = 2\n",
    "RESIDUAL = True\n",
    "BATCHNORM = True\n",
    "DUAL_OUT_DIM = 6\n",
    "model = DualModelFtype(\n",
    "        node_feature_mode=NODE_MODE,\n",
    "        gnn_type=GNN_TYPE,\n",
    "        mp_hidden_dim=GNN_HIDDEN_DIM,mp_output_dim=GNN_OUT_DIM,mp_num_layers=GNN_LAYER, \n",
    "        dual_node_mlp_hidden_dim=GNN_OUT_DIM,dual_node_mlp_output_dim=10,\n",
    "        node_mlp_num_layers=MLP_LAYER,\n",
    "        dual_edge_mlp_hidden_dim=GNN_OUT_DIM,dual_edge_mlp_output_dim=DUAL_OUT_DIM,\n",
    "        edge_mlp_num_layers=MLP_LAYER,\n",
    "        dropout_rate=DROPOUT,\n",
    "        relu_slope=0.1,\n",
    "        residual=RESIDUAL,\n",
    "        batchnorm=BATCHNORM)\n",
    "# model.load_state_dict(torch.load('./models/good/dual_model_N30-5000_SAGE_31_64_2_6_True_True.pth'))\n",
    "model.load_state_dict(torch.load('./models/good/dual_model_N50-5000_SAGE_15_64_2_6_True_True_1_1201.pth'))\n",
    "model.double()\n",
    "model.to(DEVICE)\n",
    "# Dataset\n",
    "setname = 'N100-100'\n",
    "num_graphs = 100\n",
    "dir = f'/home/hank/Datasets/QUASAR/{setname}'\n",
    "data = QUASARDataset(dir,num_graphs=num_graphs,remove_self_loops=True)\n",
    "loader = DataLoader(data,batch_size=1,shuffle=False)\n",
    "\n",
    "def rel_norm_err(A,B):\n",
    "    norm_err = torch.norm(A-B,p='fro')\n",
    "    norm_gt = torch.norm(B,p='fro')\n",
    "    return torch.div(norm_err,norm_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUAL_OUT_DIM: 6.\n",
      "Aty norm err: 0.5077, S norm err: 0.5045.\n",
      "Aty norm err: 0.5101, S norm err: 0.5056.\n",
      "Aty norm err: 0.5221, S norm err: 0.5162.\n",
      "Aty norm err: 0.5187, S norm err: 0.5135.\n",
      "Aty norm err: 0.5229, S norm err: 0.5194.\n",
      "Aty norm err: 0.5085, S norm err: 0.5040.\n",
      "Aty norm err: 0.5292, S norm err: 0.5251.\n",
      "Aty norm err: 0.5215, S norm err: 0.5171.\n",
      "Aty norm err: 0.5051, S norm err: 0.4999.\n",
      "Aty norm err: 0.5063, S norm err: 0.5012.\n",
      "Aty norm err: 0.5078, S norm err: 0.5041.\n",
      "Aty norm err: 0.5057, S norm err: 0.5015.\n",
      "Aty norm err: 0.5221, S norm err: 0.5178.\n",
      "Aty norm err: 0.4989, S norm err: 0.4947.\n",
      "Aty norm err: 0.5184, S norm err: 0.5142.\n",
      "Aty norm err: 0.5110, S norm err: 0.5067.\n",
      "Aty norm err: 0.5351, S norm err: 0.5315.\n",
      "Aty norm err: 0.5492, S norm err: 0.5443.\n",
      "Aty norm err: 0.5194, S norm err: 0.5159.\n",
      "Aty norm err: 0.5035, S norm err: 0.4995.\n",
      "Aty norm err: 0.5053, S norm err: 0.5012.\n",
      "Aty norm err: 0.4949, S norm err: 0.4903.\n",
      "Aty norm err: 0.5273, S norm err: 0.5223.\n",
      "Aty norm err: 0.5118, S norm err: 0.5065.\n",
      "Aty norm err: 0.5265, S norm err: 0.5227.\n",
      "Aty norm err: 0.5085, S norm err: 0.5035.\n",
      "Aty norm err: 0.4887, S norm err: 0.4840.\n",
      "Aty norm err: 0.5024, S norm err: 0.4984.\n",
      "Aty norm err: 0.5191, S norm err: 0.5145.\n",
      "Aty norm err: 0.4983, S norm err: 0.4940.\n",
      "Aty norm err: 0.5068, S norm err: 0.5025.\n",
      "Aty norm err: 0.5047, S norm err: 0.5003.\n",
      "Aty norm err: 0.4973, S norm err: 0.4931.\n",
      "Aty norm err: 0.5237, S norm err: 0.5187.\n",
      "Aty norm err: 0.5341, S norm err: 0.5291.\n",
      "Aty norm err: 0.5081, S norm err: 0.5034.\n",
      "Aty norm err: 0.5068, S norm err: 0.5016.\n",
      "Aty norm err: 0.5276, S norm err: 0.5228.\n",
      "Aty norm err: 0.5083, S norm err: 0.5040.\n",
      "Aty norm err: 0.5111, S norm err: 0.5064.\n",
      "Aty norm err: 0.5090, S norm err: 0.5033.\n",
      "Aty norm err: 0.5268, S norm err: 0.5215.\n",
      "Aty norm err: 0.5012, S norm err: 0.4962.\n",
      "Aty norm err: 0.5085, S norm err: 0.5035.\n",
      "Aty norm err: 0.5268, S norm err: 0.5208.\n",
      "Aty norm err: 0.4975, S norm err: 0.4924.\n",
      "Aty norm err: 0.5126, S norm err: 0.5078.\n",
      "Aty norm err: 0.5093, S norm err: 0.5033.\n",
      "Aty norm err: 0.5103, S norm err: 0.5054.\n",
      "Aty norm err: 0.5239, S norm err: 0.5182.\n",
      "Aty norm err: 0.5007, S norm err: 0.4945.\n",
      "Aty norm err: 0.5207, S norm err: 0.5146.\n",
      "Aty norm err: 0.5017, S norm err: 0.4959.\n",
      "Aty norm err: 0.5050, S norm err: 0.4998.\n",
      "Aty norm err: 0.5093, S norm err: 0.5030.\n",
      "Aty norm err: 0.4999, S norm err: 0.4937.\n",
      "Aty norm err: 0.5102, S norm err: 0.5043.\n",
      "Aty norm err: 0.4912, S norm err: 0.4853.\n",
      "Aty norm err: 0.5168, S norm err: 0.5104.\n",
      "Aty norm err: 0.5028, S norm err: 0.4970.\n",
      "Aty norm err: 0.4964, S norm err: 0.4899.\n",
      "Aty norm err: 0.5078, S norm err: 0.5021.\n",
      "Aty norm err: 0.4973, S norm err: 0.4908.\n",
      "Aty norm err: 0.5009, S norm err: 0.4947.\n",
      "Aty norm err: 0.4901, S norm err: 0.4846.\n",
      "Aty norm err: 0.5167, S norm err: 0.5106.\n",
      "Aty norm err: 0.5076, S norm err: 0.5019.\n",
      "Aty norm err: 0.4942, S norm err: 0.4885.\n",
      "Aty norm err: 0.5077, S norm err: 0.5019.\n",
      "Aty norm err: 0.5104, S norm err: 0.5035.\n",
      "Aty norm err: 0.5050, S norm err: 0.4987.\n",
      "Aty norm err: 0.5113, S norm err: 0.5045.\n",
      "Aty norm err: 0.5052, S norm err: 0.4983.\n",
      "Aty norm err: 0.4952, S norm err: 0.4891.\n",
      "Aty norm err: 0.4886, S norm err: 0.4819.\n",
      "Aty norm err: 0.5074, S norm err: 0.5009.\n",
      "Aty norm err: 0.5157, S norm err: 0.5092.\n",
      "Aty norm err: 0.4979, S norm err: 0.4916.\n",
      "Aty norm err: 0.4840, S norm err: 0.4777.\n",
      "Aty norm err: 0.5039, S norm err: 0.4970.\n",
      "Aty norm err: 0.5024, S norm err: 0.4953.\n",
      "Aty norm err: 0.5104, S norm err: 0.5032.\n",
      "Aty norm err: 0.4862, S norm err: 0.4796.\n",
      "Aty norm err: 0.4882, S norm err: 0.4815.\n",
      "Aty norm err: 0.4983, S norm err: 0.4913.\n",
      "Aty norm err: 0.4972, S norm err: 0.4905.\n",
      "Aty norm err: 0.4959, S norm err: 0.4889.\n",
      "Aty norm err: 0.5034, S norm err: 0.4964.\n",
      "Aty norm err: 0.4884, S norm err: 0.4817.\n",
      "Aty norm err: 0.4926, S norm err: 0.4859.\n",
      "Aty norm err: 0.4971, S norm err: 0.4897.\n",
      "Aty norm err: 0.4977, S norm err: 0.4906.\n",
      "Aty norm err: 0.4983, S norm err: 0.4911.\n",
      "Aty norm err: 0.5020, S norm err: 0.4948.\n",
      "Aty norm err: 0.4981, S norm err: 0.4908.\n",
      "Aty norm err: 0.4921, S norm err: 0.4845.\n",
      "Aty norm err: 0.4977, S norm err: 0.4904.\n",
      "Aty norm err: 0.4904, S norm err: 0.4833.\n",
      "Aty norm err: 0.4994, S norm err: 0.4919.\n",
      "Aty norm err: 0.4953, S norm err: 0.4881.\n"
     ]
    }
   ],
   "source": [
    "dual_pred = []\n",
    "S_norm_err = []\n",
    "Aty_norm_err = []\n",
    "print(f'DUAL_OUT_DIM: {DUAL_OUT_DIM}.')\n",
    "if DUAL_OUT_DIM == 6:\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Generate data\n",
    "        for batch in loader:\n",
    "            batch.to(DEVICE)\n",
    "            _, V, E = model(batch)\n",
    "            sol = model.recover(V,E,batch)\n",
    "            Aty_hat = sol[0]\n",
    "            C = torch.tensor(batch.C[0],dtype=torch.float64,device=Aty_hat.device)\n",
    "            S_hat = C - Aty_hat\n",
    "            Aty_gt = torch.tensor(batch.Aty[0],dtype=torch.float64,device=Aty_hat.device)\n",
    "            S_gt = torch.tensor(batch.S[0],dtype=torch.float64,device=Aty_hat.device)\n",
    "            err_Aty = rel_norm_err(Aty_hat,Aty_gt)\n",
    "            err_S = rel_norm_err(S_hat,S_gt)\n",
    "            print('Aty norm err: {:.4f}, S norm err: {:.4f}.'.format(err_Aty,err_S))\n",
    "            dual_pred.append(Aty_hat.cpu().numpy())\n",
    "            S_norm_err.append(err_S.cpu().numpy())\n",
    "            Aty_norm_err.append(err_Aty.cpu().numpy())\n",
    "elif DUAL_OUT_DIM == 16:\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Generate data\n",
    "        for batch in loader:\n",
    "            batch.to(DEVICE)\n",
    "            _, V, E = model(batch)\n",
    "            sol = model.recover(V,E,batch)\n",
    "            S_hat = sol[0]\n",
    "            C = torch.tensor(batch.C[0],dtype=torch.float64,device=S_hat.device)\n",
    "            Aty_hat = C - S_hat\n",
    "            Aty_gt = torch.tensor(batch.Aty[0],dtype=torch.float64,device=Aty_hat.device)\n",
    "            S_gt = torch.tensor(batch.S[0],dtype=torch.float64,device=Aty_hat.device)\n",
    "            err_Aty = rel_norm_err(Aty_hat,Aty_gt)\n",
    "            err_S = rel_norm_err(S_hat,S_gt)\n",
    "            print('Aty norm err: {:.4f}, S norm err: {:.4f}.'.format(err_Aty,err_S))\n",
    "            dual_pred.append(Aty_hat.cpu().numpy())\n",
    "            S_norm_err.append(err_S.cpu().numpy())\n",
    "            Aty_norm_err.append(err_Aty.cpu().numpy())\n",
    "\n",
    "S_norm_err = np.array(S_norm_err)\n",
    "Aty_norm_err = np.array(Aty_norm_err)\n",
    "mdict = {\"dual_pred\": dual_pred, \"S_norm_err\": S_norm_err, \"Aty_norm_err\": Aty_norm_err}\n",
    "# mdict = {\"Aty_norm_err\": Aty_norm_err, \"S_norm_err\": S_norm_err}\n",
    "if DUAL_OUT_DIM == 6:\n",
    "    fname = f'{setname}_dual_sol_Aty.mat'\n",
    "elif DUAL_OUT_DIM == 16:\n",
    "    fname = f'{setname}_dual_sol_S.mat'\n",
    "sio.savemat(fname,mdict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6548f5818eb0d21304c8b75b8d24fe6a88bc84b221a3961d704ebd009d182722"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gnnsdp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
