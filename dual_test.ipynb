{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import torch_geometric.nn as pyg_nn \n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "import torch.optim as optim\n",
    "import os.path as osp\n",
    "import scipy.io as sio\n",
    "from dataset import QUASARDataset\n",
    "from dual_model import DualModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data graph type: 1.\n",
      "Data graph type: 1.\n"
     ]
    }
   ],
   "source": [
    "dir = '/home/hank/Datasets/QUASAR/small'\n",
    "dataset = QUASARDataset(dir,num_graphs=100,remove_self_loops=True)\n",
    "test_dir = '/home/hank/Datasets/QUASAR/mix'\n",
    "testset = QUASARDataset(test_dir,num_graphs=200,remove_self_loops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: node_feature_mode = 1, mp_input_dim = 6, relu_slope = 0.1. GNN type: SAGE.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DualModel(\n",
       "  (mp_convs): ModuleList(\n",
       "    (0): SAGEConv(6, 64)\n",
       "    (1): SAGEConv(64, 64)\n",
       "    (2): SAGEConv(64, 64)\n",
       "    (3): SAGEConv(64, 64)\n",
       "    (4): SAGEConv(64, 64)\n",
       "    (5): SAGEConv(64, 64)\n",
       "  )\n",
       "  (dual_node_mlp): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       "  (dual_edge_mlp): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Linear(in_features=64, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GNN_TYPE = 'SAGE'\n",
    "GNN_HIDDEN_DIM = 64\n",
    "GNN_OUT_DIM = 64\n",
    "GNN_LAYER = 4\n",
    "NODE_MODE = 1\n",
    "DATA_GRAPH_TYPE = 1\n",
    "DROPOUT = 0\n",
    "MLP_LAYER = 2\n",
    "model   = DualModel(node_feature_mode=NODE_MODE,\n",
    "                     gnn_type=GNN_TYPE,\n",
    "                     mp_hidden_dim=GNN_HIDDEN_DIM,mp_output_dim=GNN_OUT_DIM,mp_num_layers=GNN_LAYER, \n",
    "                     dual_node_mlp_hidden_dim=64,dual_node_mlp_output_dim=10,\n",
    "                     node_mlp_num_layers=MLP_LAYER,\n",
    "                     dual_edge_mlp_hidden_dim=64,dual_edge_mlp_output_dim=6,\n",
    "                     edge_mlp_num_layers=MLP_LAYER, \n",
    "                     dropout_rate=DROPOUT,\n",
    "                     relu_slope=0.1)\n",
    "model.load_state_dict(torch.load('./models/dual_model_SAGE_4_64_64_1_1_2000_0.0_2.pth'))\n",
    "model.double()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss: 0.0658. # nodes: 21\n",
      "batch loss: 0.0997. # nodes: 21\n",
      "batch loss: 0.0497. # nodes: 21\n",
      "batch loss: 0.0674. # nodes: 21\n",
      "batch loss: 0.0627. # nodes: 21\n",
      "batch loss: 0.0531. # nodes: 21\n",
      "batch loss: 0.0537. # nodes: 21\n",
      "batch loss: 0.0559. # nodes: 21\n",
      "batch loss: 0.0850. # nodes: 21\n",
      "batch loss: 0.0597. # nodes: 21\n",
      "batch loss: 0.0674. # nodes: 21\n",
      "batch loss: 0.0655. # nodes: 21\n",
      "batch loss: 0.0510. # nodes: 21\n",
      "batch loss: 0.0871. # nodes: 21\n",
      "batch loss: 0.0651. # nodes: 21\n",
      "batch loss: 0.0733. # nodes: 21\n",
      "batch loss: 0.0795. # nodes: 21\n",
      "batch loss: 0.0724. # nodes: 21\n",
      "batch loss: 0.0870. # nodes: 21\n",
      "batch loss: 0.0843. # nodes: 21\n",
      "batch loss: 0.0673. # nodes: 21\n",
      "batch loss: 0.0796. # nodes: 21\n",
      "batch loss: 0.0495. # nodes: 21\n",
      "batch loss: 0.0797. # nodes: 21\n",
      "batch loss: 0.0771. # nodes: 21\n",
      "batch loss: 0.0662. # nodes: 21\n",
      "batch loss: 0.0588. # nodes: 21\n",
      "batch loss: 0.0473. # nodes: 21\n",
      "batch loss: 0.0752. # nodes: 21\n",
      "batch loss: 0.0689. # nodes: 21\n",
      "batch loss: 0.0549. # nodes: 21\n",
      "batch loss: 0.0832. # nodes: 21\n",
      "batch loss: 0.0750. # nodes: 21\n",
      "batch loss: 0.0909. # nodes: 21\n",
      "batch loss: 0.0791. # nodes: 21\n",
      "batch loss: 0.0508. # nodes: 21\n",
      "batch loss: 0.0753. # nodes: 21\n",
      "batch loss: 0.0659. # nodes: 21\n",
      "batch loss: 0.0604. # nodes: 21\n",
      "batch loss: 0.0875. # nodes: 21\n",
      "batch loss: 0.0742. # nodes: 21\n",
      "batch loss: 0.0751. # nodes: 21\n",
      "batch loss: 0.0803. # nodes: 21\n",
      "batch loss: 0.0449. # nodes: 21\n",
      "batch loss: 0.0752. # nodes: 21\n",
      "batch loss: 0.0686. # nodes: 21\n",
      "batch loss: 0.0748. # nodes: 21\n",
      "batch loss: 0.0540. # nodes: 21\n",
      "batch loss: 0.0565. # nodes: 21\n",
      "batch loss: 0.0533. # nodes: 21\n",
      "batch loss: 0.0534. # nodes: 21\n",
      "batch loss: 0.0607. # nodes: 21\n",
      "batch loss: 0.0785. # nodes: 21\n",
      "batch loss: 0.0628. # nodes: 21\n",
      "batch loss: 0.0612. # nodes: 21\n",
      "batch loss: 0.0734. # nodes: 21\n",
      "batch loss: 0.0659. # nodes: 21\n",
      "batch loss: 0.0685. # nodes: 21\n",
      "batch loss: 0.0741. # nodes: 21\n",
      "batch loss: 0.1020. # nodes: 21\n",
      "batch loss: 0.0649. # nodes: 21\n",
      "batch loss: 0.0755. # nodes: 21\n",
      "batch loss: 0.0620. # nodes: 21\n",
      "batch loss: 0.0775. # nodes: 21\n",
      "batch loss: 0.0574. # nodes: 21\n",
      "batch loss: 0.0724. # nodes: 21\n",
      "batch loss: 0.0829. # nodes: 21\n",
      "batch loss: 0.0575. # nodes: 21\n",
      "batch loss: 0.0571. # nodes: 21\n",
      "batch loss: 0.0625. # nodes: 21\n",
      "batch loss: 0.0645. # nodes: 21\n",
      "batch loss: 0.0619. # nodes: 21\n",
      "batch loss: 0.0503. # nodes: 21\n",
      "batch loss: 0.0529. # nodes: 21\n",
      "batch loss: 0.0758. # nodes: 21\n",
      "batch loss: 0.0781. # nodes: 21\n",
      "batch loss: 0.0693. # nodes: 21\n",
      "batch loss: 0.0478. # nodes: 21\n",
      "batch loss: 0.0670. # nodes: 21\n",
      "batch loss: 0.0881. # nodes: 21\n",
      "batch loss: 0.0666. # nodes: 21\n",
      "batch loss: 0.0778. # nodes: 21\n",
      "batch loss: 0.0648. # nodes: 21\n",
      "batch loss: 0.0709. # nodes: 21\n",
      "batch loss: 0.0650. # nodes: 21\n",
      "batch loss: 0.0606. # nodes: 21\n",
      "batch loss: 0.0611. # nodes: 21\n",
      "batch loss: 0.1031. # nodes: 21\n",
      "batch loss: 0.0471. # nodes: 21\n",
      "batch loss: 0.0785. # nodes: 21\n",
      "batch loss: 0.0727. # nodes: 21\n",
      "batch loss: 0.0789. # nodes: 21\n",
      "batch loss: 0.0780. # nodes: 21\n",
      "batch loss: 0.0409. # nodes: 21\n",
      "batch loss: 0.0645. # nodes: 21\n",
      "batch loss: 0.0740. # nodes: 21\n",
      "batch loss: 0.0489. # nodes: 21\n",
      "batch loss: 0.0693. # nodes: 21\n",
      "batch loss: 0.1025. # nodes: 21\n",
      "batch loss: 0.0735. # nodes: 21\n"
     ]
    }
   ],
   "source": [
    "# results on train set\n",
    "loader = DataLoader(dataset,batch_size=1,shuffle=True)\n",
    "for batch in loader:\n",
    "    _, S, Aty = model(batch)\n",
    "    dual_loss = model.loss(batch,S,Aty)\n",
    "    print('batch loss: {:.4f}. # nodes: {:d}'.format(\n",
    "                dual_loss.item(),batch.num_nodes))\n",
    "    Atyopt = batch.Aty[0]\n",
    "    Aty    = Aty[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss: 0.4130. # nodes: 21\n",
      "batch loss: 0.2597. # nodes: 21\n",
      "batch loss: 0.3967. # nodes: 31\n",
      "batch loss: 0.4582. # nodes: 31\n",
      "batch loss: 0.3249. # nodes: 21\n",
      "batch loss: 0.3989. # nodes: 31\n",
      "batch loss: 0.2399. # nodes: 21\n",
      "batch loss: 0.5908. # nodes: 31\n",
      "batch loss: 0.4067. # nodes: 31\n",
      "batch loss: 0.3520. # nodes: 21\n",
      "batch loss: 0.5084. # nodes: 31\n",
      "batch loss: 0.4294. # nodes: 31\n",
      "batch loss: 0.3257. # nodes: 21\n",
      "batch loss: 0.4061. # nodes: 31\n",
      "batch loss: 0.4345. # nodes: 31\n",
      "batch loss: 0.2260. # nodes: 21\n",
      "batch loss: 0.1885. # nodes: 21\n",
      "batch loss: 0.3833. # nodes: 21\n",
      "batch loss: 0.2188. # nodes: 21\n",
      "batch loss: 0.1755. # nodes: 21\n",
      "batch loss: 0.5114. # nodes: 31\n",
      "batch loss: 0.4558. # nodes: 31\n",
      "batch loss: 0.4728. # nodes: 31\n",
      "batch loss: 0.3957. # nodes: 31\n",
      "batch loss: 0.6119. # nodes: 31\n",
      "batch loss: 0.4698. # nodes: 31\n",
      "batch loss: 0.3938. # nodes: 31\n",
      "batch loss: 0.5417. # nodes: 31\n",
      "batch loss: 0.2695. # nodes: 21\n",
      "batch loss: 0.4996. # nodes: 31\n",
      "batch loss: 0.3964. # nodes: 31\n",
      "batch loss: 0.2748. # nodes: 21\n",
      "batch loss: 0.3789. # nodes: 31\n",
      "batch loss: 0.3841. # nodes: 31\n",
      "batch loss: 0.3861. # nodes: 31\n",
      "batch loss: 0.4051. # nodes: 31\n",
      "batch loss: 0.2464. # nodes: 21\n",
      "batch loss: 0.4449. # nodes: 31\n",
      "batch loss: 0.3266. # nodes: 21\n",
      "batch loss: 0.2743. # nodes: 21\n",
      "batch loss: 0.3264. # nodes: 21\n",
      "batch loss: 0.2629. # nodes: 21\n",
      "batch loss: 0.2529. # nodes: 21\n",
      "batch loss: 0.4579. # nodes: 31\n",
      "batch loss: 0.2759. # nodes: 21\n",
      "batch loss: 0.5676. # nodes: 31\n",
      "batch loss: 0.4133. # nodes: 31\n",
      "batch loss: 0.2862. # nodes: 21\n",
      "batch loss: 0.3463. # nodes: 31\n",
      "batch loss: 0.4807. # nodes: 31\n",
      "batch loss: 0.2612. # nodes: 21\n",
      "batch loss: 0.5177. # nodes: 31\n",
      "batch loss: 0.4276. # nodes: 21\n",
      "batch loss: 0.3023. # nodes: 21\n",
      "batch loss: 0.3948. # nodes: 31\n",
      "batch loss: 0.2910. # nodes: 21\n",
      "batch loss: 0.5208. # nodes: 21\n",
      "batch loss: 0.5274. # nodes: 31\n",
      "batch loss: 0.5043. # nodes: 31\n",
      "batch loss: 0.4303. # nodes: 31\n",
      "batch loss: 0.4826. # nodes: 31\n",
      "batch loss: 0.2275. # nodes: 21\n",
      "batch loss: 0.4223. # nodes: 31\n",
      "batch loss: 0.4473. # nodes: 31\n",
      "batch loss: 0.2726. # nodes: 21\n",
      "batch loss: 0.4463. # nodes: 31\n",
      "batch loss: 0.2713. # nodes: 21\n",
      "batch loss: 0.2575. # nodes: 21\n",
      "batch loss: 0.3576. # nodes: 21\n",
      "batch loss: 0.1712. # nodes: 21\n",
      "batch loss: 0.4913. # nodes: 31\n",
      "batch loss: 0.4577. # nodes: 31\n",
      "batch loss: 0.4302. # nodes: 31\n",
      "batch loss: 0.3821. # nodes: 31\n",
      "batch loss: 0.3225. # nodes: 21\n",
      "batch loss: 0.1275. # nodes: 21\n",
      "batch loss: 0.4005. # nodes: 31\n",
      "batch loss: 0.2493. # nodes: 21\n",
      "batch loss: 0.3339. # nodes: 21\n",
      "batch loss: 0.3308. # nodes: 21\n",
      "batch loss: 0.4006. # nodes: 31\n",
      "batch loss: 0.2755. # nodes: 21\n",
      "batch loss: 0.4206. # nodes: 21\n",
      "batch loss: 0.2574. # nodes: 21\n",
      "batch loss: 0.3366. # nodes: 21\n",
      "batch loss: 0.3919. # nodes: 21\n",
      "batch loss: 0.1785. # nodes: 21\n",
      "batch loss: 0.4217. # nodes: 31\n",
      "batch loss: 0.3489. # nodes: 21\n",
      "batch loss: 0.2609. # nodes: 21\n",
      "batch loss: 0.2350. # nodes: 21\n",
      "batch loss: 0.4565. # nodes: 31\n",
      "batch loss: 0.3297. # nodes: 21\n",
      "batch loss: 0.4358. # nodes: 31\n",
      "batch loss: 0.4391. # nodes: 31\n",
      "batch loss: 0.4261. # nodes: 31\n",
      "batch loss: 0.4384. # nodes: 31\n",
      "batch loss: 0.4434. # nodes: 31\n",
      "batch loss: 0.3735. # nodes: 21\n",
      "batch loss: 0.1835. # nodes: 21\n",
      "batch loss: 0.2562. # nodes: 21\n",
      "batch loss: 0.4590. # nodes: 31\n",
      "batch loss: 0.2120. # nodes: 21\n",
      "batch loss: 0.2024. # nodes: 21\n",
      "batch loss: 0.4438. # nodes: 31\n",
      "batch loss: 0.3897. # nodes: 31\n",
      "batch loss: 0.5101. # nodes: 31\n",
      "batch loss: 0.3288. # nodes: 21\n",
      "batch loss: 0.2585. # nodes: 21\n",
      "batch loss: 0.3697. # nodes: 21\n",
      "batch loss: 0.4454. # nodes: 31\n",
      "batch loss: 0.2579. # nodes: 21\n",
      "batch loss: 0.4156. # nodes: 31\n",
      "batch loss: 0.3984. # nodes: 21\n",
      "batch loss: 0.4286. # nodes: 31\n",
      "batch loss: 0.2719. # nodes: 21\n",
      "batch loss: 0.2793. # nodes: 21\n",
      "batch loss: 0.2587. # nodes: 21\n",
      "batch loss: 0.2408. # nodes: 21\n",
      "batch loss: 0.3465. # nodes: 21\n",
      "batch loss: 0.1442. # nodes: 21\n",
      "batch loss: 0.2854. # nodes: 21\n",
      "batch loss: 0.3053. # nodes: 21\n",
      "batch loss: 0.3599. # nodes: 21\n",
      "batch loss: 0.4270. # nodes: 31\n",
      "batch loss: 0.3952. # nodes: 21\n",
      "batch loss: 0.4165. # nodes: 31\n",
      "batch loss: 0.4108. # nodes: 31\n",
      "batch loss: 0.4380. # nodes: 31\n",
      "batch loss: 0.3941. # nodes: 21\n",
      "batch loss: 0.4651. # nodes: 21\n",
      "batch loss: 0.2485. # nodes: 21\n",
      "batch loss: 0.3904. # nodes: 31\n",
      "batch loss: 0.4929. # nodes: 31\n",
      "batch loss: 0.2349. # nodes: 21\n",
      "batch loss: 0.2578. # nodes: 21\n",
      "batch loss: 0.5128. # nodes: 31\n",
      "batch loss: 0.4220. # nodes: 31\n",
      "batch loss: 0.4656. # nodes: 31\n",
      "batch loss: 0.2337. # nodes: 21\n",
      "batch loss: 0.3958. # nodes: 31\n",
      "batch loss: 0.2632. # nodes: 21\n",
      "batch loss: 0.4231. # nodes: 31\n",
      "batch loss: 0.2732. # nodes: 21\n",
      "batch loss: 0.2525. # nodes: 21\n",
      "batch loss: 0.4058. # nodes: 31\n",
      "batch loss: 0.1808. # nodes: 21\n",
      "batch loss: 0.2987. # nodes: 21\n",
      "batch loss: 0.5598. # nodes: 21\n",
      "batch loss: 0.2222. # nodes: 21\n",
      "batch loss: 0.3424. # nodes: 21\n",
      "batch loss: 0.6211. # nodes: 31\n",
      "batch loss: 0.3785. # nodes: 31\n",
      "batch loss: 0.5521. # nodes: 31\n",
      "batch loss: 0.3984. # nodes: 31\n",
      "batch loss: 0.1750. # nodes: 21\n",
      "batch loss: 0.4987. # nodes: 31\n",
      "batch loss: 0.3031. # nodes: 21\n",
      "batch loss: 0.1791. # nodes: 21\n",
      "batch loss: 0.1862. # nodes: 21\n",
      "batch loss: 0.1569. # nodes: 21\n",
      "batch loss: 0.1419. # nodes: 21\n",
      "batch loss: 0.2422. # nodes: 21\n",
      "batch loss: 0.4065. # nodes: 31\n",
      "batch loss: 0.2660. # nodes: 21\n",
      "batch loss: 0.3766. # nodes: 31\n",
      "batch loss: 0.2057. # nodes: 21\n",
      "batch loss: 0.2062. # nodes: 21\n",
      "batch loss: 0.2663. # nodes: 21\n",
      "batch loss: 0.3936. # nodes: 31\n",
      "batch loss: 0.3285. # nodes: 21\n",
      "batch loss: 0.4155. # nodes: 31\n",
      "batch loss: 0.2900. # nodes: 21\n",
      "batch loss: 0.4119. # nodes: 31\n",
      "batch loss: 0.4109. # nodes: 31\n",
      "batch loss: 0.4273. # nodes: 31\n",
      "batch loss: 0.3170. # nodes: 21\n",
      "batch loss: 0.4228. # nodes: 31\n",
      "batch loss: 0.4419. # nodes: 31\n",
      "batch loss: 0.4017. # nodes: 31\n",
      "batch loss: 0.4146. # nodes: 31\n",
      "batch loss: 0.4364. # nodes: 31\n",
      "batch loss: 0.2769. # nodes: 21\n",
      "batch loss: 0.3877. # nodes: 31\n",
      "batch loss: 0.3971. # nodes: 31\n",
      "batch loss: 0.1930. # nodes: 21\n",
      "batch loss: 0.3207. # nodes: 21\n",
      "batch loss: 0.3979. # nodes: 31\n",
      "batch loss: 0.3980. # nodes: 31\n",
      "batch loss: 0.4047. # nodes: 31\n",
      "batch loss: 0.2231. # nodes: 21\n",
      "batch loss: 0.3985. # nodes: 31\n",
      "batch loss: 0.3953. # nodes: 31\n",
      "batch loss: 0.4528. # nodes: 31\n",
      "batch loss: 0.5741. # nodes: 31\n",
      "batch loss: 0.2328. # nodes: 21\n",
      "batch loss: 0.4171. # nodes: 31\n",
      "batch loss: 0.4556. # nodes: 31\n",
      "batch loss: 0.4285. # nodes: 31\n",
      "batch loss: 0.5186. # nodes: 31\n"
     ]
    }
   ],
   "source": [
    "# results on test set\n",
    "loader = DataLoader(testset,batch_size=1,shuffle=True)\n",
    "for batch in loader:\n",
    "    _, S, Aty = model(batch)\n",
    "    dual_loss = model.loss(batch,S,Aty)\n",
    "    print('batch loss: {:.4f}. # nodes: {:d}'.format(\n",
    "                dual_loss.item(),batch.num_nodes))\n",
    "    Atyopt = batch.Aty[0]\n",
    "    Aty    = Aty[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# int(0.5*5)\n",
    "200 % 199"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6548f5818eb0d21304c8b75b8d24fe6a88bc84b221a3961d704ebd009d182722"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gnnsdp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
