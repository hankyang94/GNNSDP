{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import torch_geometric.nn as pyg_nn \n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "import torch.optim as optim\n",
    "import os.path as osp\n",
    "import scipy.io as sio\n",
    "from dataset import QUASARDataset\n",
    "from dual_model import DualModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data graph type: 1.\n",
      "Data graph type: 1.\n"
     ]
    }
   ],
   "source": [
    "dir = '/home/hank/Datasets/QUASAR/small'\n",
    "dataset = QUASARDataset(dir,num_graphs=100,remove_self_loops=True)\n",
    "test_dir = '/home/hank/Datasets/QUASAR/mix'\n",
    "testset = QUASARDataset(test_dir,num_graphs=200,remove_self_loops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: node_feature_mode = 1, mp_input_dim = 6, relu_slope = 0.1. GNN type: SAGE.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DualModel(\n",
       "  (mp_convs): ModuleList(\n",
       "    (0): SAGEConv(6, 64)\n",
       "    (1): SAGEConv(64, 64)\n",
       "    (2): SAGEConv(64, 64)\n",
       "    (3): SAGEConv(64, 64)\n",
       "    (4): SAGEConv(64, 64)\n",
       "    (5): SAGEConv(64, 64)\n",
       "  )\n",
       "  (dual_node_mlp): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       "  (dual_edge_mlp): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GNN_TYPE = 'SAGE'\n",
    "GNN_HIDDEN_DIM = 64\n",
    "GNN_OUT_DIM = 64\n",
    "GNN_LAYER = 4\n",
    "NODE_MODE = 1\n",
    "DATA_GRAPH_TYPE = 1\n",
    "DROPOUT = 0\n",
    "MLP_LAYER = 1\n",
    "model   = DualModel(node_feature_mode=NODE_MODE,\n",
    "                     gnn_type=GNN_TYPE,\n",
    "                     mp_hidden_dim=GNN_HIDDEN_DIM,mp_output_dim=GNN_OUT_DIM,mp_num_layers=GNN_LAYER, \n",
    "                     dual_node_mlp_hidden_dim=64,dual_node_mlp_output_dim=10,\n",
    "                     node_mlp_num_layers=MLP_LAYER,\n",
    "                     dual_edge_mlp_hidden_dim=64,dual_edge_mlp_output_dim=6,\n",
    "                     edge_mlp_num_layers=MLP_LAYER, \n",
    "                     dropout_rate=DROPOUT,\n",
    "                     relu_slope=0.1)\n",
    "model.load_state_dict(torch.load('./models/dual_model_SAGE_4_64_64_1_1_1000_0.0_1.pth'))\n",
    "model.double()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss: 0.2543. # nodes: 21\n",
      "batch loss: 0.1520. # nodes: 21\n",
      "batch loss: 0.0896. # nodes: 21\n",
      "batch loss: 0.1071. # nodes: 21\n",
      "batch loss: 0.1218. # nodes: 21\n",
      "batch loss: 0.1515. # nodes: 21\n",
      "batch loss: 0.1801. # nodes: 21\n",
      "batch loss: 0.1313. # nodes: 21\n",
      "batch loss: 0.1130. # nodes: 21\n",
      "batch loss: 0.1289. # nodes: 21\n",
      "batch loss: 0.1125. # nodes: 21\n",
      "batch loss: 0.1809. # nodes: 21\n",
      "batch loss: 0.1432. # nodes: 21\n",
      "batch loss: 0.1262. # nodes: 21\n",
      "batch loss: 0.1064. # nodes: 21\n",
      "batch loss: 0.1617. # nodes: 21\n",
      "batch loss: 0.1491. # nodes: 21\n",
      "batch loss: 0.1022. # nodes: 21\n",
      "batch loss: 0.1252. # nodes: 21\n",
      "batch loss: 0.1470. # nodes: 21\n",
      "batch loss: 0.1143. # nodes: 21\n",
      "batch loss: 0.1412. # nodes: 21\n",
      "batch loss: 0.1150. # nodes: 21\n",
      "batch loss: 0.1236. # nodes: 21\n",
      "batch loss: 0.1390. # nodes: 21\n",
      "batch loss: 0.1743. # nodes: 21\n",
      "batch loss: 0.1277. # nodes: 21\n",
      "batch loss: 0.1102. # nodes: 21\n",
      "batch loss: 0.1837. # nodes: 21\n",
      "batch loss: 0.2076. # nodes: 21\n",
      "batch loss: 0.1360. # nodes: 21\n",
      "batch loss: 0.1355. # nodes: 21\n",
      "batch loss: 0.1327. # nodes: 21\n",
      "batch loss: 0.1188. # nodes: 21\n",
      "batch loss: 0.1447. # nodes: 21\n",
      "batch loss: 0.1605. # nodes: 21\n",
      "batch loss: 0.1753. # nodes: 21\n",
      "batch loss: 0.1167. # nodes: 21\n",
      "batch loss: 0.2640. # nodes: 21\n",
      "batch loss: 0.0828. # nodes: 21\n",
      "batch loss: 0.0924. # nodes: 21\n",
      "batch loss: 0.1604. # nodes: 21\n",
      "batch loss: 0.1811. # nodes: 21\n",
      "batch loss: 0.1145. # nodes: 21\n",
      "batch loss: 0.1007. # nodes: 21\n",
      "batch loss: 0.1219. # nodes: 21\n",
      "batch loss: 0.1453. # nodes: 21\n",
      "batch loss: 0.1315. # nodes: 21\n",
      "batch loss: 0.1701. # nodes: 21\n",
      "batch loss: 0.1741. # nodes: 21\n",
      "batch loss: 0.1520. # nodes: 21\n",
      "batch loss: 0.1771. # nodes: 21\n",
      "batch loss: 0.1371. # nodes: 21\n",
      "batch loss: 0.1306. # nodes: 21\n",
      "batch loss: 0.0921. # nodes: 21\n",
      "batch loss: 0.1054. # nodes: 21\n",
      "batch loss: 0.1714. # nodes: 21\n",
      "batch loss: 0.1159. # nodes: 21\n",
      "batch loss: 0.1080. # nodes: 21\n",
      "batch loss: 0.1480. # nodes: 21\n",
      "batch loss: 0.1058. # nodes: 21\n",
      "batch loss: 0.1149. # nodes: 21\n",
      "batch loss: 0.1529. # nodes: 21\n",
      "batch loss: 0.1255. # nodes: 21\n",
      "batch loss: 0.1310. # nodes: 21\n",
      "batch loss: 0.1387. # nodes: 21\n",
      "batch loss: 0.1546. # nodes: 21\n",
      "batch loss: 0.1321. # nodes: 21\n",
      "batch loss: 0.1242. # nodes: 21\n",
      "batch loss: 0.1560. # nodes: 21\n",
      "batch loss: 0.1424. # nodes: 21\n",
      "batch loss: 0.1386. # nodes: 21\n",
      "batch loss: 0.1326. # nodes: 21\n",
      "batch loss: 0.0978. # nodes: 21\n",
      "batch loss: 0.1082. # nodes: 21\n",
      "batch loss: 0.1395. # nodes: 21\n",
      "batch loss: 0.1298. # nodes: 21\n",
      "batch loss: 0.1337. # nodes: 21\n",
      "batch loss: 0.1631. # nodes: 21\n",
      "batch loss: 0.1572. # nodes: 21\n",
      "batch loss: 0.1504. # nodes: 21\n",
      "batch loss: 0.1297. # nodes: 21\n",
      "batch loss: 0.0880. # nodes: 21\n",
      "batch loss: 0.1151. # nodes: 21\n",
      "batch loss: 0.1486. # nodes: 21\n",
      "batch loss: 0.1071. # nodes: 21\n",
      "batch loss: 0.1060. # nodes: 21\n",
      "batch loss: 0.1515. # nodes: 21\n",
      "batch loss: 0.1358. # nodes: 21\n",
      "batch loss: 0.1331. # nodes: 21\n",
      "batch loss: 0.1436. # nodes: 21\n",
      "batch loss: 0.1233. # nodes: 21\n",
      "batch loss: 0.1851. # nodes: 21\n",
      "batch loss: 0.1190. # nodes: 21\n",
      "batch loss: 0.2386. # nodes: 21\n",
      "batch loss: 0.1721. # nodes: 21\n",
      "batch loss: 0.1551. # nodes: 21\n",
      "batch loss: 0.1443. # nodes: 21\n",
      "batch loss: 0.1485. # nodes: 21\n",
      "batch loss: 0.1000. # nodes: 21\n"
     ]
    }
   ],
   "source": [
    "# results on train set\n",
    "loader = DataLoader(dataset,batch_size=1,shuffle=True)\n",
    "for batch in loader:\n",
    "    _, S, Aty = model(batch)\n",
    "    dual_loss = model.loss(batch,S,Aty)\n",
    "    print('batch loss: {:.4f}. # nodes: {:d}'.format(\n",
    "                dual_loss.item(),batch.num_nodes))\n",
    "    Atyopt = batch.Aty[0]\n",
    "    Aty    = Aty[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss: 0.6130. # nodes: 31\n",
      "batch loss: 0.2773. # nodes: 21\n",
      "batch loss: 0.2339. # nodes: 21\n",
      "batch loss: 0.4168. # nodes: 21\n",
      "batch loss: 0.4802. # nodes: 31\n",
      "batch loss: 0.4362. # nodes: 31\n",
      "batch loss: 0.4837. # nodes: 31\n",
      "batch loss: 0.3869. # nodes: 31\n",
      "batch loss: 0.3570. # nodes: 21\n",
      "batch loss: 0.5005. # nodes: 31\n",
      "batch loss: 0.3714. # nodes: 21\n",
      "batch loss: 0.4667. # nodes: 31\n",
      "batch loss: 0.3175. # nodes: 21\n",
      "batch loss: 0.4146. # nodes: 31\n",
      "batch loss: 0.3681. # nodes: 31\n",
      "batch loss: 0.5458. # nodes: 31\n",
      "batch loss: 0.4150. # nodes: 31\n",
      "batch loss: 0.3896. # nodes: 21\n",
      "batch loss: 0.4336. # nodes: 31\n",
      "batch loss: 0.3741. # nodes: 21\n",
      "batch loss: 0.2590. # nodes: 21\n",
      "batch loss: 0.4190. # nodes: 21\n",
      "batch loss: 0.4479. # nodes: 21\n",
      "batch loss: 0.3304. # nodes: 21\n",
      "batch loss: 0.3403. # nodes: 31\n",
      "batch loss: 0.3642. # nodes: 21\n",
      "batch loss: 0.4077. # nodes: 31\n",
      "batch loss: 0.2939. # nodes: 21\n",
      "batch loss: 0.3461. # nodes: 21\n",
      "batch loss: 0.5160. # nodes: 31\n",
      "batch loss: 0.3954. # nodes: 31\n",
      "batch loss: 0.4397. # nodes: 21\n",
      "batch loss: 0.2760. # nodes: 21\n",
      "batch loss: 0.4572. # nodes: 31\n",
      "batch loss: 0.2195. # nodes: 21\n",
      "batch loss: 0.4075. # nodes: 21\n",
      "batch loss: 0.2674. # nodes: 21\n",
      "batch loss: 0.3771. # nodes: 31\n",
      "batch loss: 0.3684. # nodes: 21\n",
      "batch loss: 0.3344. # nodes: 21\n",
      "batch loss: 0.3695. # nodes: 31\n",
      "batch loss: 0.2562. # nodes: 21\n",
      "batch loss: 0.3499. # nodes: 21\n",
      "batch loss: 0.3785. # nodes: 21\n",
      "batch loss: 0.4213. # nodes: 31\n",
      "batch loss: 0.3288. # nodes: 21\n",
      "batch loss: 0.5881. # nodes: 31\n",
      "batch loss: 0.4040. # nodes: 31\n",
      "batch loss: 0.3459. # nodes: 21\n",
      "batch loss: 0.3393. # nodes: 21\n",
      "batch loss: 0.5065. # nodes: 21\n",
      "batch loss: 0.2601. # nodes: 21\n",
      "batch loss: 0.4246. # nodes: 21\n",
      "batch loss: 0.4366. # nodes: 21\n",
      "batch loss: 0.3863. # nodes: 31\n",
      "batch loss: 0.4114. # nodes: 21\n",
      "batch loss: 0.2699. # nodes: 21\n",
      "batch loss: 0.4004. # nodes: 31\n",
      "batch loss: 0.4394. # nodes: 31\n",
      "batch loss: 0.4004. # nodes: 31\n",
      "batch loss: 0.4560. # nodes: 31\n",
      "batch loss: 0.4376. # nodes: 31\n",
      "batch loss: 0.3662. # nodes: 31\n",
      "batch loss: 0.3078. # nodes: 21\n",
      "batch loss: 0.2288. # nodes: 21\n",
      "batch loss: 0.4633. # nodes: 31\n",
      "batch loss: 0.4932. # nodes: 31\n",
      "batch loss: 0.3700. # nodes: 31\n",
      "batch loss: 0.4280. # nodes: 31\n",
      "batch loss: 0.4125. # nodes: 31\n",
      "batch loss: 0.2305. # nodes: 21\n",
      "batch loss: 0.5148. # nodes: 31\n",
      "batch loss: 0.3889. # nodes: 31\n",
      "batch loss: 0.4719. # nodes: 31\n",
      "batch loss: 0.3766. # nodes: 21\n",
      "batch loss: 0.3971. # nodes: 21\n",
      "batch loss: 0.5044. # nodes: 21\n",
      "batch loss: 0.2884. # nodes: 21\n",
      "batch loss: 0.3764. # nodes: 21\n",
      "batch loss: 0.2636. # nodes: 21\n",
      "batch loss: 0.3970. # nodes: 21\n",
      "batch loss: 0.3880. # nodes: 31\n",
      "batch loss: 0.4297. # nodes: 21\n",
      "batch loss: 0.3474. # nodes: 21\n",
      "batch loss: 0.4131. # nodes: 31\n",
      "batch loss: 0.2603. # nodes: 21\n",
      "batch loss: 0.4345. # nodes: 31\n",
      "batch loss: 0.3081. # nodes: 21\n",
      "batch loss: 0.4046. # nodes: 31\n",
      "batch loss: 0.2927. # nodes: 21\n",
      "batch loss: 0.3062. # nodes: 21\n",
      "batch loss: 0.3186. # nodes: 21\n",
      "batch loss: 0.4466. # nodes: 31\n",
      "batch loss: 0.4246. # nodes: 31\n",
      "batch loss: 0.4101. # nodes: 31\n",
      "batch loss: 0.2932. # nodes: 21\n",
      "batch loss: 0.4858. # nodes: 31\n",
      "batch loss: 0.4201. # nodes: 31\n",
      "batch loss: 0.3336. # nodes: 21\n",
      "batch loss: 0.3526. # nodes: 21\n",
      "batch loss: 0.3966. # nodes: 31\n",
      "batch loss: 0.4364. # nodes: 31\n",
      "batch loss: 0.2192. # nodes: 21\n",
      "batch loss: 0.4222. # nodes: 31\n",
      "batch loss: 0.6640. # nodes: 21\n",
      "batch loss: 0.4182. # nodes: 31\n",
      "batch loss: 0.3235. # nodes: 21\n",
      "batch loss: 0.5281. # nodes: 31\n",
      "batch loss: 0.4501. # nodes: 21\n",
      "batch loss: 0.4816. # nodes: 31\n",
      "batch loss: 0.4436. # nodes: 31\n",
      "batch loss: 0.1769. # nodes: 21\n",
      "batch loss: 0.3676. # nodes: 31\n",
      "batch loss: 0.4684. # nodes: 31\n",
      "batch loss: 0.3241. # nodes: 21\n",
      "batch loss: 0.3918. # nodes: 31\n",
      "batch loss: 0.2771. # nodes: 21\n",
      "batch loss: 0.4003. # nodes: 31\n",
      "batch loss: 0.2034. # nodes: 21\n",
      "batch loss: 0.5152. # nodes: 31\n",
      "batch loss: 0.4132. # nodes: 31\n",
      "batch loss: 0.3850. # nodes: 31\n",
      "batch loss: 0.4871. # nodes: 31\n",
      "batch loss: 0.4583. # nodes: 31\n",
      "batch loss: 0.5651. # nodes: 31\n",
      "batch loss: 0.2985. # nodes: 21\n",
      "batch loss: 0.4647. # nodes: 31\n",
      "batch loss: 0.3951. # nodes: 31\n",
      "batch loss: 0.4853. # nodes: 21\n",
      "batch loss: 0.3427. # nodes: 21\n",
      "batch loss: 0.4133. # nodes: 31\n",
      "batch loss: 0.3952. # nodes: 31\n",
      "batch loss: 0.2512. # nodes: 21\n",
      "batch loss: 0.4524. # nodes: 31\n",
      "batch loss: 0.4355. # nodes: 31\n",
      "batch loss: 0.5076. # nodes: 21\n",
      "batch loss: 0.4274. # nodes: 31\n",
      "batch loss: 0.3820. # nodes: 21\n",
      "batch loss: 0.3913. # nodes: 31\n",
      "batch loss: 0.4308. # nodes: 31\n",
      "batch loss: 0.4527. # nodes: 31\n",
      "batch loss: 0.3636. # nodes: 21\n",
      "batch loss: 0.4917. # nodes: 31\n",
      "batch loss: 0.3890. # nodes: 31\n",
      "batch loss: 0.4352. # nodes: 21\n",
      "batch loss: 0.4421. # nodes: 31\n",
      "batch loss: 0.2088. # nodes: 21\n",
      "batch loss: 0.3672. # nodes: 31\n",
      "batch loss: 0.5307. # nodes: 31\n",
      "batch loss: 0.2952. # nodes: 21\n",
      "batch loss: 0.2990. # nodes: 21\n",
      "batch loss: 0.4022. # nodes: 21\n",
      "batch loss: 0.4515. # nodes: 31\n",
      "batch loss: 0.2906. # nodes: 21\n",
      "batch loss: 0.3019. # nodes: 21\n",
      "batch loss: 0.4662. # nodes: 21\n",
      "batch loss: 0.4088. # nodes: 31\n",
      "batch loss: 0.3958. # nodes: 31\n",
      "batch loss: 0.3890. # nodes: 21\n",
      "batch loss: 0.3599. # nodes: 21\n",
      "batch loss: 0.2593. # nodes: 21\n",
      "batch loss: 0.4734. # nodes: 31\n",
      "batch loss: 0.4120. # nodes: 21\n",
      "batch loss: 0.4108. # nodes: 21\n",
      "batch loss: 0.4393. # nodes: 31\n",
      "batch loss: 0.5001. # nodes: 31\n",
      "batch loss: 0.5490. # nodes: 31\n",
      "batch loss: 0.3794. # nodes: 31\n",
      "batch loss: 0.4073. # nodes: 31\n",
      "batch loss: 0.3806. # nodes: 31\n",
      "batch loss: 0.3837. # nodes: 31\n",
      "batch loss: 0.3555. # nodes: 21\n",
      "batch loss: 0.4429. # nodes: 31\n",
      "batch loss: 0.2288. # nodes: 21\n",
      "batch loss: 0.2572. # nodes: 21\n",
      "batch loss: 0.4147. # nodes: 31\n",
      "batch loss: 0.3650. # nodes: 21\n",
      "batch loss: 0.1499. # nodes: 21\n",
      "batch loss: 0.4691. # nodes: 31\n",
      "batch loss: 0.5329. # nodes: 31\n",
      "batch loss: 0.5487. # nodes: 31\n",
      "batch loss: 0.3618. # nodes: 21\n",
      "batch loss: 0.2444. # nodes: 21\n",
      "batch loss: 0.3914. # nodes: 31\n",
      "batch loss: 0.4110. # nodes: 31\n",
      "batch loss: 0.2830. # nodes: 21\n",
      "batch loss: 0.3788. # nodes: 21\n",
      "batch loss: 0.3818. # nodes: 31\n",
      "batch loss: 0.3235. # nodes: 21\n",
      "batch loss: 0.4089. # nodes: 31\n",
      "batch loss: 0.2276. # nodes: 21\n",
      "batch loss: 0.5675. # nodes: 31\n",
      "batch loss: 0.3191. # nodes: 21\n",
      "batch loss: 0.4005. # nodes: 21\n",
      "batch loss: 0.3117. # nodes: 21\n",
      "batch loss: 0.2654. # nodes: 21\n",
      "batch loss: 0.4060. # nodes: 21\n",
      "batch loss: 0.5813. # nodes: 31\n",
      "batch loss: 0.2726. # nodes: 21\n",
      "batch loss: 0.4315. # nodes: 21\n"
     ]
    }
   ],
   "source": [
    "# results on test set\n",
    "loader = DataLoader(testset,batch_size=1,shuffle=True)\n",
    "for batch in loader:\n",
    "    _, S, Aty = model(batch)\n",
    "    dual_loss = model.loss(batch,S,Aty)\n",
    "    print('batch loss: {:.4f}. # nodes: {:d}'.format(\n",
    "                dual_loss.item(),batch.num_nodes))\n",
    "    Atyopt = batch.Aty[0]\n",
    "    Aty    = Aty[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6548f5818eb0d21304c8b75b8d24fe6a88bc84b221a3961d704ebd009d182722"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gnnsdp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
