{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import torch_geometric.nn as pyg_nn \n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "import torch.optim as optim\n",
    "import os.path as osp\n",
    "import scipy.io as sio\n",
    "from dataset import QUASARDataset\n",
    "from primal_model import PrimalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/home/hank/Datasets/QUASAR/small'\n",
    "dataset = QUASARDataset(dir,num_graphs=100,remove_self_loops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: node_feature_mode = 1, mp_input_dim = 6, relu_slope = 0.1. GNN type: SAGE.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PrimalModel(\n",
       "  (mp_convs): ModuleList(\n",
       "    (0): SAGEConv(6, 64)\n",
       "    (1): SAGEConv(64, 64)\n",
       "    (2): SAGEConv(64, 64)\n",
       "    (3): SAGEConv(64, 64)\n",
       "    (4): SAGEConv(64, 64)\n",
       "    (5): SAGEConv(64, 64)\n",
       "  )\n",
       "  (primal_node_mlp): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       "  (primal_edge_mlp): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GNN_TYPE = 'SAGE'\n",
    "GNN_HIDDEN_DIM = 64\n",
    "GNN_OUT_DIM = 64\n",
    "GNN_LAYER = 4\n",
    "NODE_MODE = 1\n",
    "DATA_GRAPH_TYPE = 1\n",
    "DROPOUT = 0.2\n",
    "MLP_LAYER = 1\n",
    "model   = PrimalModel(node_feature_mode=NODE_MODE,\n",
    "                     gnn_type=GNN_TYPE,\n",
    "                     mp_hidden_dim=GNN_HIDDEN_DIM,mp_output_dim=GNN_OUT_DIM,mp_num_layers=GNN_LAYER, \n",
    "                     primal_node_mlp_hidden_dim=64,primal_node_mlp_output_dim=10,\n",
    "                     node_mlp_num_layers=MLP_LAYER,\n",
    "                     primal_edge_mlp_hidden_dim=64,primal_edge_mlp_output_dim=10,\n",
    "                     edge_mlp_num_layers=MLP_LAYER, \n",
    "                     dropout_rate=DROPOUT,\n",
    "                     relu_slope=0.1)\n",
    "model.load_state_dict(torch.load('./models/primal_model_SAGE_4_64_64_1_1_1000_0_1.pth'))\n",
    "model.double()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss: 0.0937.\n",
      "batch loss: 0.0512.\n",
      "batch loss: 0.0386.\n",
      "batch loss: 0.1086.\n",
      "batch loss: 0.0488.\n",
      "batch loss: 0.1014.\n",
      "batch loss: 0.0587.\n",
      "batch loss: 0.0942.\n",
      "batch loss: 0.0940.\n",
      "batch loss: 0.0631.\n",
      "batch loss: 0.1092.\n",
      "batch loss: 0.0386.\n",
      "batch loss: 0.1228.\n",
      "batch loss: 0.0508.\n",
      "batch loss: 0.0957.\n",
      "batch loss: 0.1143.\n",
      "batch loss: 0.0390.\n",
      "batch loss: 0.1035.\n",
      "batch loss: 0.0468.\n",
      "batch loss: 0.0753.\n",
      "batch loss: 0.0605.\n",
      "batch loss: 0.0408.\n",
      "batch loss: 0.0745.\n",
      "batch loss: 0.0534.\n",
      "batch loss: 0.1288.\n",
      "batch loss: 0.0574.\n",
      "batch loss: 0.0500.\n",
      "batch loss: 0.0440.\n",
      "batch loss: 0.0559.\n",
      "batch loss: 0.0994.\n",
      "batch loss: 0.0958.\n",
      "batch loss: 0.0252.\n",
      "batch loss: 0.0768.\n",
      "batch loss: 0.0651.\n",
      "batch loss: 0.0315.\n",
      "batch loss: 0.1294.\n",
      "batch loss: 0.0493.\n",
      "batch loss: 0.0679.\n",
      "batch loss: 0.0812.\n",
      "batch loss: 0.0794.\n",
      "batch loss: 0.0965.\n",
      "batch loss: 0.1216.\n",
      "batch loss: 0.0480.\n",
      "batch loss: 0.0715.\n",
      "batch loss: 0.0772.\n",
      "batch loss: 0.0631.\n",
      "batch loss: 0.1045.\n",
      "batch loss: 0.0523.\n",
      "batch loss: 0.0621.\n",
      "batch loss: 0.0526.\n",
      "batch loss: 0.0812.\n",
      "batch loss: 0.1274.\n",
      "batch loss: 0.0522.\n",
      "batch loss: 0.0911.\n",
      "batch loss: 0.0758.\n",
      "batch loss: 0.0742.\n",
      "batch loss: 0.0757.\n",
      "batch loss: 0.0344.\n",
      "batch loss: 0.1001.\n",
      "batch loss: 0.3534.\n",
      "batch loss: 0.0509.\n",
      "batch loss: 0.0708.\n",
      "batch loss: 0.0619.\n",
      "batch loss: 0.1110.\n",
      "batch loss: 0.0662.\n",
      "batch loss: 0.0410.\n",
      "batch loss: 0.0521.\n",
      "batch loss: 0.0451.\n",
      "batch loss: 0.0612.\n",
      "batch loss: 0.0693.\n",
      "batch loss: 0.0336.\n",
      "batch loss: 0.0509.\n",
      "batch loss: 0.1803.\n",
      "batch loss: 0.0846.\n",
      "batch loss: 0.0890.\n",
      "batch loss: 0.0584.\n",
      "batch loss: 0.0623.\n",
      "batch loss: 0.0330.\n",
      "batch loss: 0.1182.\n",
      "batch loss: 0.0700.\n",
      "batch loss: 0.0792.\n",
      "batch loss: 0.0596.\n",
      "batch loss: 0.0963.\n",
      "batch loss: 0.0452.\n",
      "batch loss: 0.0685.\n",
      "batch loss: 0.0731.\n",
      "batch loss: 0.0519.\n",
      "batch loss: 0.0339.\n",
      "batch loss: 0.0901.\n",
      "batch loss: 0.0588.\n",
      "batch loss: 0.0625.\n",
      "batch loss: 0.0759.\n",
      "batch loss: 0.0724.\n",
      "batch loss: 0.0660.\n",
      "batch loss: 0.1287.\n",
      "batch loss: 0.0445.\n",
      "batch loss: 0.0681.\n",
      "batch loss: 0.0839.\n",
      "batch loss: 0.0867.\n",
      "batch loss: 0.0700.\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset,batch_size=1,shuffle=True)\n",
    "for batch in loader:\n",
    "    xorg = batch.x\n",
    "    x, X = model(batch)\n",
    "    primal_loss = model.loss(batch,X)\n",
    "    print('batch loss: {:.4f}.'.format(\n",
    "                primal_loss.item()))\n",
    "    X = X[0]\n",
    "    Xopt = batch.X[0]\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6548f5818eb0d21304c8b75b8d24fe6a88bc84b221a3961d704ebd009d182722"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gnnsdp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
