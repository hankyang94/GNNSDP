{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import torch_geometric.nn as pyg_nn \n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "import torch.optim as optim\n",
    "import os.path as osp\n",
    "import scipy.io as sio\n",
    "from dataset import QUASARDataset\n",
    "from primal_model import PrimalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data graph type: 1.\n",
      "Data graph type: 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected # graphs: 200. Actual # graphs: 200.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dir = '/home/hank/Datasets/QUASAR/small'\n",
    "dataset = QUASARDataset(dir,num_graphs=100,remove_self_loops=True)\n",
    "test_dir = '/home/hank/Datasets/QUASAR/mix'\n",
    "testset = QUASARDataset(test_dir,num_graphs=200,remove_self_loops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: node_feature_mode = 1, mp_input_dim = 6, relu_slope = 0.1. GNN type: SAGE.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PrimalModel(\n",
       "  (mp_convs): ModuleList(\n",
       "    (0): SAGEConv(6, 64)\n",
       "    (1): SAGEConv(64, 64)\n",
       "    (2): SAGEConv(64, 64)\n",
       "    (3): SAGEConv(64, 64)\n",
       "    (4): SAGEConv(64, 64)\n",
       "    (5): SAGEConv(64, 64)\n",
       "  )\n",
       "  (primal_node_mlp): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       "  (primal_edge_mlp): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GNN_TYPE = 'SAGE'\n",
    "GNN_HIDDEN_DIM = 64\n",
    "GNN_OUT_DIM = 64\n",
    "GNN_LAYER = 4\n",
    "NODE_MODE = 1\n",
    "DATA_GRAPH_TYPE = 1\n",
    "DROPOUT = 0\n",
    "MLP_LAYER = 2\n",
    "model   = PrimalModel(node_feature_mode=NODE_MODE,\n",
    "                     gnn_type=GNN_TYPE,\n",
    "                     mp_hidden_dim=GNN_HIDDEN_DIM,mp_output_dim=GNN_OUT_DIM,mp_num_layers=GNN_LAYER, \n",
    "                     primal_node_mlp_hidden_dim=64,primal_node_mlp_output_dim=10,\n",
    "                     node_mlp_num_layers=MLP_LAYER,\n",
    "                     primal_edge_mlp_hidden_dim=64,primal_edge_mlp_output_dim=10,\n",
    "                     edge_mlp_num_layers=MLP_LAYER, \n",
    "                     dropout_rate=DROPOUT,\n",
    "                     relu_slope=0.1)\n",
    "model.load_state_dict(torch.load('./models/primal_model_SAGE_4_64_64_1_1_1000_0.0_2.pth'))\n",
    "model.double()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hank/miniconda3/envs/gnnsdp/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss: 0.0219.\n",
      "batch loss: 0.0515.\n",
      "batch loss: 0.4335.\n",
      "batch loss: 0.0777.\n",
      "batch loss: 0.0629.\n",
      "batch loss: 0.0476.\n",
      "batch loss: 0.0529.\n",
      "batch loss: 0.0604.\n",
      "batch loss: 0.0528.\n",
      "batch loss: 0.0867.\n",
      "batch loss: 0.0546.\n",
      "batch loss: 0.0475.\n",
      "batch loss: 0.0450.\n",
      "batch loss: 0.0388.\n",
      "batch loss: 0.0404.\n",
      "batch loss: 0.0300.\n",
      "batch loss: 0.0997.\n",
      "batch loss: 0.0815.\n",
      "batch loss: 0.1194.\n",
      "batch loss: 0.0530.\n",
      "batch loss: 0.0523.\n",
      "batch loss: 0.0761.\n",
      "batch loss: 0.0556.\n",
      "batch loss: 0.0638.\n",
      "batch loss: 0.0721.\n",
      "batch loss: 0.0469.\n",
      "batch loss: 0.0648.\n",
      "batch loss: 0.0523.\n",
      "batch loss: 0.0473.\n",
      "batch loss: 0.0451.\n",
      "batch loss: 0.0535.\n",
      "batch loss: 0.0366.\n",
      "batch loss: 0.0776.\n",
      "batch loss: 0.0793.\n",
      "batch loss: 0.0416.\n",
      "batch loss: 0.0789.\n",
      "batch loss: 0.0451.\n",
      "batch loss: 0.0918.\n",
      "batch loss: 0.0734.\n",
      "batch loss: 0.0496.\n",
      "batch loss: 0.0475.\n",
      "batch loss: 0.0683.\n",
      "batch loss: 0.0677.\n",
      "batch loss: 0.0232.\n",
      "batch loss: 0.1393.\n",
      "batch loss: 0.0998.\n",
      "batch loss: 0.0626.\n",
      "batch loss: 0.0725.\n",
      "batch loss: 0.0517.\n",
      "batch loss: 0.0658.\n",
      "batch loss: 0.0853.\n",
      "batch loss: 0.0636.\n",
      "batch loss: 0.0572.\n",
      "batch loss: 0.0841.\n",
      "batch loss: 0.0377.\n",
      "batch loss: 0.0609.\n",
      "batch loss: 0.0448.\n",
      "batch loss: 0.0494.\n",
      "batch loss: 0.0411.\n",
      "batch loss: 0.0546.\n",
      "batch loss: 0.0572.\n",
      "batch loss: 0.0318.\n",
      "batch loss: 0.0677.\n",
      "batch loss: 0.0796.\n",
      "batch loss: 0.0525.\n",
      "batch loss: 0.0803.\n",
      "batch loss: 0.0416.\n",
      "batch loss: 0.0972.\n",
      "batch loss: 0.0662.\n",
      "batch loss: 0.0541.\n",
      "batch loss: 0.0484.\n",
      "batch loss: 0.0511.\n",
      "batch loss: 0.0487.\n",
      "batch loss: 0.0518.\n",
      "batch loss: 0.0481.\n",
      "batch loss: 0.0595.\n",
      "batch loss: 0.0521.\n",
      "batch loss: 0.0595.\n",
      "batch loss: 0.0484.\n",
      "batch loss: 0.0559.\n",
      "batch loss: 0.0366.\n",
      "batch loss: 0.0466.\n",
      "batch loss: 0.0611.\n",
      "batch loss: 0.1148.\n",
      "batch loss: 0.0615.\n",
      "batch loss: 0.0398.\n",
      "batch loss: 0.0419.\n",
      "batch loss: 0.0448.\n",
      "batch loss: 0.0756.\n",
      "batch loss: 0.0509.\n",
      "batch loss: 0.0624.\n",
      "batch loss: 0.0699.\n",
      "batch loss: 0.0466.\n",
      "batch loss: 0.0369.\n",
      "batch loss: 0.0630.\n",
      "batch loss: 0.0625.\n",
      "batch loss: 0.0653.\n",
      "batch loss: 0.0799.\n",
      "batch loss: 0.0515.\n",
      "batch loss: 0.0421.\n"
     ]
    }
   ],
   "source": [
    "# results on train set\n",
    "loader = DataLoader(dataset,batch_size=1,shuffle=True)\n",
    "for batch in loader:\n",
    "    xorg = batch.x\n",
    "    x, X = model(batch)\n",
    "    primal_loss = model.loss(batch,X)\n",
    "    print('batch loss: {:.4f}.'.format(\n",
    "                primal_loss.item()))\n",
    "    X = X[0]\n",
    "    Xopt = batch.X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss: 1.1055, # nodes: 31\n",
      "batch loss: 1.0099, # nodes: 31\n",
      "batch loss: 0.6909, # nodes: 31\n",
      "batch loss: 1.0999, # nodes: 21\n",
      "batch loss: 1.1855, # nodes: 21\n",
      "batch loss: 1.0193, # nodes: 31\n",
      "batch loss: 1.0875, # nodes: 21\n",
      "batch loss: 0.9957, # nodes: 21\n",
      "batch loss: 1.1677, # nodes: 21\n",
      "batch loss: 1.0508, # nodes: 31\n",
      "batch loss: 1.1411, # nodes: 31\n",
      "batch loss: 1.1745, # nodes: 21\n",
      "batch loss: 0.8419, # nodes: 31\n",
      "batch loss: 0.9663, # nodes: 31\n",
      "batch loss: 1.1386, # nodes: 21\n",
      "batch loss: 1.1529, # nodes: 21\n",
      "batch loss: 1.2428, # nodes: 31\n",
      "batch loss: 1.0744, # nodes: 31\n",
      "batch loss: 1.2386, # nodes: 21\n",
      "batch loss: 0.9069, # nodes: 31\n",
      "batch loss: 1.1059, # nodes: 21\n",
      "batch loss: 1.1685, # nodes: 21\n",
      "batch loss: 1.1222, # nodes: 31\n",
      "batch loss: 1.0433, # nodes: 31\n",
      "batch loss: 1.1852, # nodes: 21\n",
      "batch loss: 1.0296, # nodes: 21\n",
      "batch loss: 1.1383, # nodes: 21\n",
      "batch loss: 1.0608, # nodes: 31\n",
      "batch loss: 1.2631, # nodes: 31\n",
      "batch loss: 1.2420, # nodes: 31\n",
      "batch loss: 1.2987, # nodes: 31\n",
      "batch loss: 1.0507, # nodes: 31\n",
      "batch loss: 1.0841, # nodes: 21\n",
      "batch loss: 1.0643, # nodes: 21\n",
      "batch loss: 1.0320, # nodes: 31\n",
      "batch loss: 1.0540, # nodes: 31\n",
      "batch loss: 0.9857, # nodes: 21\n",
      "batch loss: 1.1378, # nodes: 21\n",
      "batch loss: 0.5737, # nodes: 31\n",
      "batch loss: 1.1250, # nodes: 31\n",
      "batch loss: 0.6265, # nodes: 21\n",
      "batch loss: 1.1391, # nodes: 31\n",
      "batch loss: 1.0886, # nodes: 21\n",
      "batch loss: 0.7925, # nodes: 21\n",
      "batch loss: 0.9408, # nodes: 21\n",
      "batch loss: 1.2268, # nodes: 31\n",
      "batch loss: 1.2591, # nodes: 31\n",
      "batch loss: 1.1863, # nodes: 31\n",
      "batch loss: 1.1313, # nodes: 21\n",
      "batch loss: 1.2333, # nodes: 31\n",
      "batch loss: 0.9723, # nodes: 21\n",
      "batch loss: 1.0458, # nodes: 31\n",
      "batch loss: 1.0849, # nodes: 31\n",
      "batch loss: 1.0934, # nodes: 21\n",
      "batch loss: 1.2003, # nodes: 31\n",
      "batch loss: 0.9328, # nodes: 31\n",
      "batch loss: 1.0440, # nodes: 31\n",
      "batch loss: 1.1852, # nodes: 21\n",
      "batch loss: 1.1041, # nodes: 31\n",
      "batch loss: 1.1134, # nodes: 21\n",
      "batch loss: 1.1285, # nodes: 31\n",
      "batch loss: 1.2269, # nodes: 21\n",
      "batch loss: 1.0447, # nodes: 21\n",
      "batch loss: 1.0583, # nodes: 31\n",
      "batch loss: 1.0194, # nodes: 31\n",
      "batch loss: 0.1113, # nodes: 31\n",
      "batch loss: 1.0636, # nodes: 21\n",
      "batch loss: 1.2125, # nodes: 21\n",
      "batch loss: 1.0163, # nodes: 21\n",
      "batch loss: 1.0998, # nodes: 21\n",
      "batch loss: 0.9160, # nodes: 31\n",
      "batch loss: 1.0445, # nodes: 21\n",
      "batch loss: 0.1875, # nodes: 31\n",
      "batch loss: 1.1408, # nodes: 21\n",
      "batch loss: 0.8564, # nodes: 31\n",
      "batch loss: 0.6103, # nodes: 21\n",
      "batch loss: 1.0562, # nodes: 31\n",
      "batch loss: 0.9356, # nodes: 31\n",
      "batch loss: 1.1477, # nodes: 21\n",
      "batch loss: 0.9213, # nodes: 31\n",
      "batch loss: 1.0553, # nodes: 31\n",
      "batch loss: 1.1729, # nodes: 21\n",
      "batch loss: 1.0513, # nodes: 21\n",
      "batch loss: 1.0635, # nodes: 31\n",
      "batch loss: 0.9922, # nodes: 31\n",
      "batch loss: 1.2175, # nodes: 21\n",
      "batch loss: 1.0455, # nodes: 31\n",
      "batch loss: 0.3398, # nodes: 21\n",
      "batch loss: 0.9656, # nodes: 21\n",
      "batch loss: 1.1113, # nodes: 31\n",
      "batch loss: 0.8041, # nodes: 21\n",
      "batch loss: 1.0429, # nodes: 31\n",
      "batch loss: 1.0789, # nodes: 21\n",
      "batch loss: 1.0922, # nodes: 21\n",
      "batch loss: 1.2429, # nodes: 31\n",
      "batch loss: 1.0832, # nodes: 31\n",
      "batch loss: 1.2046, # nodes: 31\n",
      "batch loss: 0.4499, # nodes: 31\n",
      "batch loss: 0.6795, # nodes: 21\n",
      "batch loss: 1.0926, # nodes: 31\n",
      "batch loss: 1.1866, # nodes: 21\n",
      "batch loss: 1.0432, # nodes: 21\n",
      "batch loss: 1.1035, # nodes: 31\n",
      "batch loss: 1.1848, # nodes: 31\n",
      "batch loss: 0.8140, # nodes: 21\n",
      "batch loss: 1.0420, # nodes: 31\n",
      "batch loss: 0.8641, # nodes: 31\n",
      "batch loss: 1.2096, # nodes: 21\n",
      "batch loss: 1.2379, # nodes: 21\n",
      "batch loss: 1.0827, # nodes: 21\n",
      "batch loss: 1.0005, # nodes: 21\n",
      "batch loss: 1.0461, # nodes: 21\n",
      "batch loss: 0.8283, # nodes: 21\n",
      "batch loss: 1.0413, # nodes: 31\n",
      "batch loss: 0.3349, # nodes: 31\n",
      "batch loss: 1.1180, # nodes: 31\n",
      "batch loss: 0.7028, # nodes: 21\n",
      "batch loss: 1.1136, # nodes: 21\n",
      "batch loss: 1.2159, # nodes: 31\n",
      "batch loss: 1.1528, # nodes: 31\n",
      "batch loss: 1.1240, # nodes: 31\n",
      "batch loss: 0.9872, # nodes: 31\n",
      "batch loss: 1.1092, # nodes: 21\n",
      "batch loss: 1.1333, # nodes: 21\n",
      "batch loss: 1.2797, # nodes: 31\n",
      "batch loss: 1.0054, # nodes: 21\n",
      "batch loss: 1.1877, # nodes: 31\n",
      "batch loss: 1.2196, # nodes: 21\n",
      "batch loss: 1.0688, # nodes: 31\n",
      "batch loss: 1.1583, # nodes: 21\n",
      "batch loss: 0.6043, # nodes: 21\n",
      "batch loss: 1.2836, # nodes: 21\n",
      "batch loss: 0.7725, # nodes: 21\n",
      "batch loss: 0.6641, # nodes: 21\n",
      "batch loss: 0.5022, # nodes: 31\n",
      "batch loss: 1.1157, # nodes: 31\n",
      "batch loss: 1.2537, # nodes: 21\n",
      "batch loss: 1.0042, # nodes: 31\n",
      "batch loss: 0.9694, # nodes: 31\n",
      "batch loss: 1.0885, # nodes: 21\n",
      "batch loss: 1.0934, # nodes: 21\n",
      "batch loss: 1.0154, # nodes: 21\n",
      "batch loss: 1.0444, # nodes: 31\n",
      "batch loss: 0.8956, # nodes: 31\n",
      "batch loss: 1.0949, # nodes: 21\n",
      "batch loss: 1.1014, # nodes: 21\n",
      "batch loss: 1.1988, # nodes: 31\n",
      "batch loss: 1.0889, # nodes: 21\n",
      "batch loss: 1.0874, # nodes: 31\n",
      "batch loss: 0.3531, # nodes: 21\n",
      "batch loss: 0.9223, # nodes: 31\n",
      "batch loss: 1.1209, # nodes: 31\n",
      "batch loss: 1.0218, # nodes: 31\n",
      "batch loss: 1.2102, # nodes: 31\n",
      "batch loss: 0.9020, # nodes: 31\n",
      "batch loss: 1.0822, # nodes: 31\n",
      "batch loss: 1.0490, # nodes: 31\n",
      "batch loss: 0.9201, # nodes: 31\n",
      "batch loss: 1.0481, # nodes: 31\n",
      "batch loss: 1.0537, # nodes: 21\n",
      "batch loss: 1.1539, # nodes: 21\n",
      "batch loss: 1.0475, # nodes: 31\n",
      "batch loss: 0.8408, # nodes: 31\n",
      "batch loss: 1.3946, # nodes: 21\n",
      "batch loss: 0.8119, # nodes: 31\n",
      "batch loss: 1.1061, # nodes: 31\n",
      "batch loss: 1.0937, # nodes: 21\n",
      "batch loss: 0.7945, # nodes: 21\n",
      "batch loss: 1.2682, # nodes: 21\n",
      "batch loss: 1.1397, # nodes: 31\n",
      "batch loss: 0.4996, # nodes: 31\n",
      "batch loss: 1.0930, # nodes: 21\n",
      "batch loss: 1.3326, # nodes: 21\n",
      "batch loss: 0.9223, # nodes: 21\n",
      "batch loss: 1.0550, # nodes: 31\n",
      "batch loss: 1.1558, # nodes: 21\n",
      "batch loss: 1.0790, # nodes: 21\n",
      "batch loss: 0.8246, # nodes: 31\n",
      "batch loss: 1.1951, # nodes: 21\n",
      "batch loss: 0.3278, # nodes: 21\n",
      "batch loss: 1.0850, # nodes: 21\n",
      "batch loss: 0.6954, # nodes: 21\n",
      "batch loss: 0.9836, # nodes: 31\n",
      "batch loss: 1.2157, # nodes: 21\n",
      "batch loss: 1.5368, # nodes: 31\n",
      "batch loss: 1.1485, # nodes: 21\n",
      "batch loss: 1.0721, # nodes: 31\n",
      "batch loss: 1.0543, # nodes: 21\n",
      "batch loss: 1.1142, # nodes: 31\n",
      "batch loss: 0.9799, # nodes: 21\n",
      "batch loss: 1.1255, # nodes: 21\n",
      "batch loss: 0.1450, # nodes: 21\n",
      "batch loss: 1.0622, # nodes: 31\n",
      "batch loss: 1.0275, # nodes: 21\n",
      "batch loss: 1.0466, # nodes: 31\n",
      "batch loss: 0.9056, # nodes: 21\n",
      "batch loss: 0.6930, # nodes: 21\n",
      "batch loss: 0.9379, # nodes: 31\n",
      "batch loss: 1.0788, # nodes: 21\n",
      "batch loss: 1.1154, # nodes: 21\n"
     ]
    }
   ],
   "source": [
    "# results on test set\n",
    "loader = DataLoader(testset,batch_size=1,shuffle=True)\n",
    "for batch in loader:\n",
    "    xorg = batch.x\n",
    "    x, X = model(batch)\n",
    "    primal_loss = model.loss(batch,X)\n",
    "    print('batch loss: {:.4f}, # nodes: {:d}'.format(\n",
    "                primal_loss.item(),batch.num_nodes))\n",
    "    X = X[0]\n",
    "    Xopt = batch.X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/p_model_SAGE_4_2_2000.pth\n"
     ]
    }
   ],
   "source": [
    "GNN_TYPE = 'SAGE'\n",
    "GNN_HIDDEN_DIM = 64\n",
    "GNN_OUT_DIM = 64\n",
    "GNN_LAYER = 4\n",
    "NODE_MODE = 1\n",
    "DATA_GRAPH_TYPE = 1\n",
    "NUM_EPOCHES = 2000\n",
    "DROPOUT = 0.0\n",
    "MLP_LAYER = 2\n",
    "pfname = \"./models/p_model_{}_{}_{}_{}.pth\".format(\n",
    "    GNN_TYPE,GNN_LAYER,MLP_LAYER,NUM_EPOCHES)\n",
    "print(pfname)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6548f5818eb0d21304c8b75b8d24fe6a88bc84b221a3961d704ebd009d182722"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gnnsdp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
